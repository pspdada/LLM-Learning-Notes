# CS231n

课程：[CS231n](https://www.bilibili.com/video/BV1nJ411z7fe/)

- Relative resources
    
    [https://github.com/Na-moe/CS231n-2024](https://github.com/Na-moe/CS231n-2024)
    
    PPT：[Index of /slides/2017 (stanford.edu)](https://cs231n.stanford.edu/slides/2017/)
    
    最新：[Stanford University CS231n: Deep Learning for Computer Vision](https://cs231n.stanford.edu/schedule.html)
    

## Lecture 1 Intro

Computer vision is the study of visual data.

CS231n focuses on one of the most important problems of visual recognition - **image classification (图像分类)**

There is a number of visual recognition problems that are related to image classification, such as object detection (物体检测), image captioning (图像描述).

Convolutional Neural Networks (CNN) have become an important tool for object recognition

- Our philosophy
    - Thorough and Detailed.
        - Understand how to write from scratch, debug and train convolutional neural networks.
    - Practical.
        - Focus on practical techniques for training these networks at scale, and on GPUs(e.g. will touch on distributed optimization, differences between CPU vs. GPU, etc.)
        - Also look at state of the art software tools such as Caffe. TensorFlow,and (Py)Torch
    - State of the art.
        - Most materials are new from research world in the past 1-3 years. Very exciting stuff!
    - Fun.
        - Some fun topics such as lmage Captioning (using RNN)
        - Also DeepDream, NeuralStyle, etc.
- Pre-requisite
    - Proficiency in Python, some high-level familiarity with C/C++
        - All class assignments will be in Python (and use numpy), but some of the deep learning libraries we may look at later in the class are written in C++.
        - A Python tutorial available on course website
    - College Calculus, Linear Algebra
    - Equivalent knowledge of CS229 (Machine Learning)
        - We will be formulating cost functions, taking derivatives and performing ptimization with gradient descent.

## Lecture 2 Image Classification pipeline

Image Classification: A core task in Computer Vision

- Challenges
    - Viewpoint variation
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled.png)
        
    - Illumination
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%201.png)
        
    - Occlusion
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%202.png)
        
    - Background Clutter
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%203.png)
        

### An image classifier

- Hard-code rules
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%204.png)
    
- Data-Driven Approach
    - Steps
        - Collect a dataset of images and labels
        - use Machine Learning to train a dassifier
        - Evaluate the classifier on new images
    - Nearest Neighbor Classification
        
        (最临近算法)
        
        hyperparameters (choices about the algorithm that we set rather than learn): $K$ and the distance metric
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%205.png)
        
        - Distance Metric to compare images
            
            The L1 distance depends on your choice of coordinates system. So if you rotate the coordinate frame, that will change the L1 distance between points (doing this won’t influence the L2 distance)
            
            If there some individual entries of the input features vector have important meanings, L1 might be a more natural fit.
            
            However, if the features are just a generic vector in space, use L2 distance
            
            - L1 (Manhattan) distance $L1(\mathbf{x}, \mathbf{y}) = \sum\limits_{i} |x_i - y_i|$
                
                ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%206.png)
                
                ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%207.png)
                
            - L2 (Euclidean 欧氏距离) distance $L2(\mathbf{x}, \mathbf{y}) = \sqrt{\sum\limits_{i=1}^n (x_i - y_i)^2}$
            对每个维度上的坐标值做差，然后将这些差值的平方求和，最后对这个和开平方根。这个过程实质上计算的是两点间直线距离的概念，在机器学习和统计学中常用于度量数据点之间的相似性。
            公式稍作变形： 对于测试点集中的每一个点 $x_i$ 和训练点集中的每一个点 $x'_j$
            $d(\mathbf{x}_i, \mathbf{x}'_j) = \sqrt{\sum\limits_{k=1}^{n}(x_{ik}^2 + x_{jk}'^2 - 2x_{ik}x'_{jk})}=\sqrt{\sum\limits_{k=1}^{n}x_{ik}^2 + \sum\limits_{k=1}^{n}x_{jk}'^2 -\sum\limits_{k=1}^{n} 2x_{ik}x'_{jk}}$
                - $x_{ik}$ 是测试点 $i$ 在第 $k$ 个维度的坐标
                - $x_{jk}'$ 是训练点 $j$ 在第 $k$ 个维度的坐标
                - $n$ 是空间的维度
                
                ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%208.png)
                
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%209.png)
            
        - Nearest Neighbor Classifier
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2010.png)
            
            注：计算 distances 这一行使用了 numpy 的广播机制
            
            Q: With N examples, how fast are training and prediction?
            A: Train $O(1)$, predict $O(N)$
            This is bad: we want classifiers that are fast at prediction; slow for training is ok
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2011.png)
            
            - The decision regions of a nearest neighbor classifier
            the points are training set, different colors represent different categories (class lable)
                
                ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2012.png)
                
        - K-Nearest Neighbors (KNN, K-近邻算法)
            
            Instead of copying label from the nearest neighbor, take **maiority vote**  (多数投票) from $K$ closest neighbor points
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2013.png)
            
        - Setting Hyperparameters
            
            Keep a very strict separation between the val data and the test data is important.
            
            Choose hyperparameters using the **validation set**; only run on the test set once at the very end.
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2014.png)
            
            Our algorithm is able to see the labels of the training set, but for val set, algorithm can’t see the labels, we only use the labels of the val set to check the performance of our algorithm.
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2015.png)
            
        - Disadvantages
            - Very slow at test time
            - **Distance metrics on pixels are not informative**
                
                ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2016.png)
                
            - Curse of dimensionality
            We hope our training examples to cover the space densely (Otherwise the nearest neighbors could be quite far away from the testing points) 
            → we need a number of training examples, which is exponential (指数) in the dimension of the problem.
                
                ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2017.png)
                
                (different colors in the picture mean different categories of the training set)
                
    - Linear Classification
        
        (线性分类)
        
        - Parametric Approach
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2018.png)
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2019.png)
            
            - Linear Classification is a **template matching approach**.
                
                The **linear classifier only learns one template for each class**. 
                
                Each of the rows in the matrix $W$ correspond to the template of a class.
                
                (Neural networks and other more complex models don’t have the restriction of just learning a single template per category.)
                
        - Interpreting a Linear Classifier
            - Take the rows of weight matrix $W$ and unravel it back into an image to visualize:
                
                ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2020.png)
                
            - Linear Classification is learning linear decision boundaries between pixels in a high dimensional space
                
                ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2021.png)
                
        - Hard cases for a linear classifier
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2022.png)
            

## Lecture 3 Loss function and Optimization

### Loss function

- **Multiclass SVM loss**
    
    Multiclass SVM loss：多分类 SVM 损失函数
    
    Given an example $(x_i, y_i)$, where $x_i$ is the image and $y_i$ is the real (integer) label (1~10)
    
    using the shorthand for the scores vector: $s = f(x_i, W)$
    
    the SVM loss (also called Hinge loss): 
    
    $$
    \begin{align*}
    L_i &= \sum_{j \neq y_i} \begin{cases} 0
     & \text{if } s_{y_i} \geq s_j + \Delta \\
      s_j - s_{y_i} + \Delta   & \text{otherwise}
    \end{cases} \\
    &= \sum\limits_{j \neq y_i} \max(0, s_j - s_{y_i} + \Delta)
    \end{align*}
    
    $$
    
    $s_j$ 是分类器预测出来的第 $j$ 个类别的分数， $s_{y_i}$ 是预测出来的正确类别（即标签所属类别）的分数， $\Delta$ 是阈值（safety margin）通常为 1
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2023.png)
    
    - Python implement
        
        ```python
        import numpy as np
        
        def L_i_vectorized(x: np.ndarray, y: int, W: np.ndarray) -> float:
            """
            计算单个样本i的损失函数值，采用向量化方法实现。
            
            参数:
            - x: 特征向量，类型为numpy.ndarray，表示输入数据样本。
            - y: 标签索引，类型为int，表示样本的真实标签在分类中的索引。
            - W: 权重矩阵，类型为numpy.ndarray，表示模型的权重参数。
            
            返回:
            - loss_i: 单个样本i的损失值，类型为float。
            """
            # 计算线性分数
            scores = W.dot(x)
            
            # 计算边际损失，注意正确分类的边际损失强制设为0
            margins = np.maximum(0, scores - scores[y] + 1)
            margins[y] = 0
            
            # 计算损失
            loss_i = np.sum(margins)
            
            return loss_i
        ```
        
    - Q&A:
        - the min of Multiclass SVM loss is **0** (预测出的正确类别的分数比预测出的其他类别的分数都高出阈值) and the max is **infinity**
        - Q: At initialization $W$ is small so all $s \approx 0$, What is the loss?
        A: $L =(c-1)\Delta$, where $c$ is the number of classes
        - Q: What if the sum when calculating $L_i$ was over all classes (including $j=y_i$)?
        A: The $L_i$ increases by $\Delta$ (正确类别的 $L_i=0+\Delta=\Delta$)
        - Q: What if we used mean instead of sum when calculating $L_i$?
        A: Doesn’t change, beacuse we actually don’t care about the true values of the loss.
        - Q: What if we used $L_i= \sum\limits_{j \neq y_i} \max(0, s_j - s_{y_i} + \Delta)^2$
        A: Different. The point of a loss function is to quantify how bad are different categories of mistakes,
        - Q: Suppose that we found a $W$ such that $L=0$, is this $W$ unique?
        A: No, $2W$ is also lead to $L=0$ (the margins between the correct and incorrect scores will also double, since they were already greater than $\Delta$, they still greater than $\Delta$ now)
    - gradient $dW$
        
        对于 $s = f(x_i; W) = Wx_i$
        
        对于 $s_j$ 的计算，只有当 $x_{ik}$（即 $x_i$ 的第 $k$ 个元素）与权重矩阵 $W$ 的对应元素相乘时才会贡献，即 $s_j = \sum\limits_{k=1}^{D} W_{jk}x_{ik}$，因此 $\frac{\partial s_j}{\partial W_{jk}} = x_{ik}$。对于其他的 $W_{lm}$（ $l \neq j$ 或 $m \neq k$），这个偏导数为零，因为它们不影响 $s_j$
        
- Regularization term
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2024.png)
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2025.png)
    
    Regularization hyper-parameter $\lambda$ trades off between the two
    
    - Intuition
        
        When doing a regression problem in terms of different polynomial basis functions, maybe the model has access to polynomials of very high degree, by adding this regression penalty, can encourage the model to prefer polynomials of lower degree, if they fit the data properly.
        
        If the model want to use these more complex item (like high degrees of polynomial), it need to overcome the penalty.
        
    - Types
        - L2 regularization $R(W) = \sum\limits_k \sum\limits_l W_{k,l}^2$
        实现：np.sum(W * W)
        - L1 regularization $R(W) = \sum\limits_k \sum\limits_l |W_{k,l}|$
        - Elastic net (L1 + L2) $R(W) = \sum\limits_k \sum\limits_l \beta W_{k,l}^2 + |W_{k,l}|$
        - Max norm regularization (might see later)
        - Dropout (will see later)
        - Fancier: Batch normalization, stochastic depth
    - Difference between L2 and L1 regularization
        - L2 regularization prefers to spread the influence across all the different values in feature $x$, the decisions tend to spread out and depend on the entire $x$ vector.
        - L1 regularization prefers sparse (稀疏) solutions that can drives most of the entries of $W$ to 0 and allows a few to deviate from 0.
- Softmax Classifier
    
    also called **Multinomial Logistic Regression**
    
    - scores: $s=f(x_i;W)$
    scores we get by linear classification is **unnormalized log probabilities** of the classes, $s$ 是一个标量值，表示网络（或模型）在权重 $W$ 对输入 $x_i$ 处理后得到的对于类别 $k$ 的未归一化得分（logit）
    - $P(Y=k|X=x_i) = \frac{e^{s_k}}{\sum\limits_{j=1}^C e^{s_j}}$ 
    表示在给定输入特征向量 $x_i$ 的条件下，预测类别 $Y$ 为第 $k$ 类的概率，
    - $L_i=-\log P(Y=y_i|X=x_i)$
    目标是最大化整个数据集的似然（likelihood），或者等效地，最小化负对数似然损失
    - In summary: $L_i=-\log\left(\frac{e^{s_{y_i}}}{\sum_{j=1}^C e^{s_j}}\right)= -s_{y_i} + \log\left(\sum\limits_{j=1}^C e^{s_j}\right)$
    
    这个表达式鼓励模型提高正确分类的得分，同时相对降低其他分类的得分，从而达到分类的目的
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2026.png)
    
    - Q&A:
        - theoretically, the min of loss is **0** and the max is **plus infinity**
        - Q: At initialization $W$ is small so all $s \approx 0$, What is the loss?
        A: $L=-log(\frac{1}{c})=log(c)$, where $c$ is the number of classes
    - gradient
        - $L-s$
            
            $L_i$对于 $s_{y_i}$ 的偏导数 $\frac{\partial L_i}{\partial s_{y_i}} = -1 + \frac{e^{s_{y_i}}}{\sum\limits_{j=1}^C e^{s_j}} =  -1 + P(Y=y_i|X=x_i)$
            
            - 具体求法
                
                当然，让我们一步步来解析这个导数的推导过程。已知损失函数 $L_i$ 如下：$L_i = -\log\left(\frac{e^{s_{y_i}}}{\sum_{j=1}^C e^{s_j}}\right)$，记 $u = \frac{e^{s_{y_i}}}{\sum_{j=1}^C e^{s_j}}$
                
                那么有： $\frac{\partial L_i}{\partial s_{y_i}} = \frac{\partial}{\partial s_{y_i}}(-\log(u)) = -\frac{1}{u} \cdot \frac{\partial u}{\partial s_{y_i}}$
                
                接下来，我们需要计算 $u$ 关于 $s_{y_i}$ 的导数，注意到 $\frac{\partial}{\partial s_{y_i}}(s_{y_i}) = 1$ 而 $\frac{\partial}{\partial s_{y_i}}(e^{s_j}) = 0$ 对于所有 $j \neq y_i$，
                
                $\frac{\partial u}{\partial s_{y_i}} = \frac{(\sum\limits_{j=1}^C e^{s_j} )\cdot e^{s_{y_i}} \cdot \frac{\partial}{\partial s_{y_i}}(s_{y_i}) - e^{s_{y_i}} \cdot  \frac{\partial}{\partial s_{y_i}}(\sum\limits_{j=1}^C e^{s_j})}{(\sum\limits_{j=1}^C e^{s_j})^2}= \frac{e^{s_{y_i}} \cdot \sum\limits_{j=1}^C e^{s_j} - e^{s_{y_i}} \cdot e^{s_{y_i}}}{(\sum\limits_{j=1}^C e^{s_j})^2} = \frac{e^{s_{y_i}}}={\sum\limits_{j=1}^C e^{s_j}} - \frac{e^{2s_{y_i}}}{(\sum\limits_{j=1}^C e^{s_j})^2}=\frac{e^{s_{y_i}}}{\sum\limits_{j=1}^C e^{s_j}} \cdot \left(1 - \frac{e^{s_{y_i}}}{\sum\limits_{j=1}^C e^{s_j}}\right)$
                
                这实际上就是：$\frac{\partial u}{\partial s_{y_i}} = u \cdot (1 - u)$
                
                回到原始的导数计算中，代入 $u$ 和 $\frac{\partial u}{\partial s_{y_i}}$：$\frac{\partial L_i}{\partial s_{y_i}} = -\frac{1}{u} \cdot u \cdot (1 - u)  = -1 + u$
                
            
            $j \neq y_i$ 时， $\frac{\partial L_i}{\partial s_j} = \frac{1}{\sum\limits_{m=1}^C e^{s_m}} \cdot e^{s_j} = P(Y=j|X=x_i)$
            
        - $L - W$
            
            对于 $s = f(x_i; W) = Wx_i$
            
            对于 $s_j$ 的计算，只有当 $x_{ik}$（即 $x_i$ 的第 $k$ 个元素）与权重矩阵 $W$ 的对应元素相乘时才会贡献，即 $s_j = \sum\limits_{k=1}^{D} W_{jk}x_{ik}$，因此 $\frac{\partial s_j}{\partial W_{jk}} = x_{ik}$。对于其他的 $W_{lm}$（ $l \neq j$ 或 $m \neq k$），这个偏导数为零，因为它们不影响 $s_j$
            
            应用链式法则 $\frac{\partial L_i}{\partial W_{jk}}=\frac{\partial L_i}{\partial s_k} \cdot \frac{\partial s_j}{\partial W_{jk}}$
            
            对于 $j =y_i$，  $\frac{\partial L_i}{\partial w_{jk}} = x_{ik} \cdot (p_{y_i}-1 )$
            
            对于 $j \neq y_i$， $\frac{\partial L_i}{\partial w_{kj}} = x_{ik} \cdot p_j$
            
            汇总，得到整个数据集上的平均梯度（逐元素形式）
            
            $\frac{\partial L}{\partial w_{jk}} = \frac{1}{N} \sum\limits_{i=1}^{N} \left( x_{ik} (p_j - \textbf{1}_{j=y_i}) \right)$
            
            即 $\frac{\partial L}{\partial W} = \frac{1}{N} \cdot X^T \cdot (P - Y)$
            
            - 推导
                - 符号意义:
                    - $X$ 是一个 $N \times D$ 的矩阵，其中每一行代表一个样本的特征向量 $x_i$
                    - $P$ 是一个 $N \times C$ 的矩阵，每一行包含样本在所有类别上的预测概率向量 $(p_1, p_2, ..., p_C)$
                    - $Y$ 是一个 $N \times C$ 的独热编码矩阵，如果第 $i$ 个样本属于第 $c$ 类，则 $Y_{ic}=1$，其他位置为 $0$
                    - $w_{jk}$ 是权重矩阵 $W$ 中的一个元素，对应于从第 $k$ 个特征到第 $j$ 个类别的权重
                - 逐元素推导到矩阵运算
                    - 在逐元素公式中， $\frac{\partial L}{\partial w_{jk}}$ 表示权重 $W$ 的第 $jk$ 个元素关于损失函数 $L$ 的偏导数，它是通过对所有样本 $i$ 的某个特定操作求和得到的。
                    - 对于所有的 $j$ 和 $k$，该操作涉及到乘以 $x_{ik}$（第 $i$ 个样本的第 $k$ 个特征值），然后乘以 $(p_j - \textbf{1}_{j=y_i})$，后者是第 $j$ 类的概率减去一个指示函数（若 $j$ 是样本 $i$ 的真实类别则为1，否则为0）。
                - 转换到矩阵乘法:
                    - 关键在于理解 $P - Y$ 这一步。$$P$$ 是所有样本在所有类别上的预测概率矩阵，而 $Y$ 是独热编码的标签矩阵。 $P - Y$ 实际上是为每一行（每个样本）创建了一个向量，其中每个元素表示该类别的预测概率减去该样本是否属于该类别的标记（即 $p_j - \textbf{1}_{j=y_i}$）。
                    - 由于我们需要对所有样本的相同特征 $k$ 做这项操作，我们可以将 $X$ 的转置 $X^T$ 与 $(P - Y)$ 相乘。$$X^T$$ 的每一列就是特征 $k$ 在所有样本上的值，与 $(P - Y)$ 相乘，相当于对所有样本执行上述逐元素操作并沿特征维度求和。
- Difference between the two loss functions
    - One aspect:
        
        The difference is how we choose to interpret those scores, to quantitatively measure the badness afterwards.
        
        - For SVM loss, we look at the margins between the scores of the correct class and those of the incorrect class.
        - For cross-entropy loss, we compute a probability distriution, and then look at the **minus log probability** of the correct class.
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2027.png)
        
    - Another aspect
        - The SVM loss only care about getting correct score to be greater than a margin above the incorrect scores → get one data point over the bar and then doesn’t care about this data point any more.
        - The softmax loss always drive the score of the correct class towards infinity and the score of the incorrect classes down towards minus infinity → always try to **continually improve every single data point** to get better.
- Recap
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2028.png)
    

### Optimization

- Follow the slope
    
    In 1-dimension, the derivative of a function: $\frac{df(x)}{dx} = \lim\limits_{{h \to 0}} \frac{f(x+h) - f(x)}{h}$
    
    In multiple dimensions, the gradient is the vector of partial derivatives along each dimension
    The slope in any direction is the dot product of the unit vector of that direction with the gradient
    The direction of steepest descent is the nenative gradient
    
- Use calculus to compute an analytic gradient
    - Numerical gradient (有限差分估计): approximate, slow, easy to write
    - Analytic gradient (解析梯度计算): exact, fast, error-prone
    - In practice: Always use analytic gradient, but check implementation with numerical gradient. This is called a gradient check.
- Stochastic Gradient Descent (SGD)
    
    At every iteration, we sample some small set of training examples, called a **mini-batch**, to compute an estimate of the full sum and the true gradient.
    
    Commonly 32/64/128 examples per mini-batch
    

## Lecture 4 Backpropagation and Neural Networks

### Computational graphs and **backpropagation**

How to compute the analytic gradient for arbitrarily complex functions?

Using a framework called **computational graphs** to express a function, then we can use **backpropagation**, which recursively use the **chain rule** to compute the gradient with respect to every variable in the computational graph.

![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2029.png)

What we need to deal with is just the **local gradient**, then use the chain rule to numerically multiply this all the way backwards and get the gradients of all the parameters.

- example
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2030.png)
    
    - Computational graph representation may not be unique
        
        Choose one where local gradients at each node can be easily expressed.
        
        Sigmoid local gradient: $\frac{d\sigma(x)}{dx} = \frac{e^{-x}}{(1+e^{-x})^2} = \left(\frac{1+e^{-x}-1}{1+e^{-x}}\right)\left(\frac{1}{1+e^{-x}}\right) = (1-\sigma(x))\sigma(x)$
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2031.png)
        
- Patterns in backward flow
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2032.png)
    
- Gradients for vector
    
    when $x,y,z$ are vectors, $\frac{\partial z}{\partial x},\frac{\partial z}{\partial y}$ are Jacobian matrix now
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2033.png)
    
    雅可比矩阵（Jacobian matrix）是向量值函数的一阶偏导数组成的矩阵。对于一个从 $\mathbb{R}^n$ 到 $\mathbb{R}^m$ 的函数 $\mathbf{f} : \mathbb{R}^n \rightarrow \mathbb{R}^m$，其雅可比矩阵 $\mathbf{J}$ 是一个 $m \times n$ 的矩阵。矩阵中的第 $i$ 行第 $i$ 列元素是函数 $\mathbf{f}$ 的第 $i$ 个分量函数 $f_i$ 对第 $j$ 个变量 $x_j$ 的偏导数。
    
    - example:  ReLU 激活函数
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2034.png)
        
        $\frac{\partial L}{\partial x} = \frac{\partial L}{\partial f} \cdot \frac{\partial f}{\partial x}$ where $\mathbf{J}=\frac{\partial f}{\partial x}$ is a 4096 x 4096 Jacobian matrix
        
        对于这个例子：
        
        $\mathbf{J} = \begin{pmatrix}
        \frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} & \cdots & \frac{\partial f_1}{\partial x_{4096}} \\
        \frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2} & \cdots & \frac{\partial f_2}{\partial x_{4096}} \\
        \vdots & \vdots & \ddots & \vdots \\
        \frac{\partial f_{4096}}{\partial x_1} & \frac{\partial f_{4096}}{\partial x_2} & \cdots & \frac{\partial f_{4096}}{\partial x_{4096}}
        \end{pmatrix}$
        
        $\mathbf{J}$ is a diagonal matrix since each element of output $f_i(x)$ is affected by the corresponding element of input $x_i$.
        
        $\frac{\partial f_i}{\partial x_i} =
        \begin{cases}
        1, & \text{if } x_i > 0 \\
        0, & \text{if } x_i \leq 0
        \end{cases}$
        
    - example: $L_2$ function
        
        $f(x, W) = \|W \cdot x\|^2 = \sum\limits_{i=1}^n (W\cdot{x})_i^2$
        where $x \in \mathbb{R}^n,\ W \in \mathbb{R}^{n \times n}$
        
        中间过程：
        
        $q = W \cdot x =
        \begin{pmatrix}
        W_{1,1} x_1 + \cdots + W_{1,n} x_n \\
        \vdots \\
        W_{n,1} x_1 + \cdots + W_{n,n} x_n
        \end{pmatrix}=
        \begin{pmatrix}
        q_1 \\
        \vdots \\
        q_n
        \end{pmatrix},\ f(q) = \|q\|^2 = q_1^2 + \cdots + q_n^2$
        
        求偏导： 
        
        $\frac{\partial f}{\partial q_i} = \frac{\partial}{\partial q_i} (q_1^2 + q_2^2 + \cdots + q_i^2 + \cdots + q_n^2)= 2q_i$  ⇒ $\nabla_q f=2q$
        
        即：$L2$ 范数平方函数关于其输入向量的梯度是该输入向量的两倍
        
        $\frac{\partial q_k}{\partial W_{i,j}} = \textbf{1}_{k=i}x_j$
        
        $\textbf{1}_{k=i}$ 是一个指示函数，当 $𝑘=𝑖$ 时值为 1，否则为 0
        
        $\frac{\partial f}{\partial W_{i,j}} = \sum\limits_k \left(\frac{\partial f}{\partial q_k}\cdot \frac{\partial q_k}{\partial W_{i,j}}\right) = \sum\limits_k (2q_k) (1_{k=i}x_j) = 2q_i x_j$ ⇒ $\nabla_wf = 2q \cdot x^T$
        
        “求和”确保了所有影响 $f$ 值的路径都被考虑进来
        
        $\frac{\partial q_k}{\partial x_i} = W_{k,i}$
        
        $\frac{\partial f}{\partial x_i} = \sum\limits_k \frac{\partial f}{\partial q_k} \frac{\partial q_k}{\partial x_i} = \sum\limits_k 2q_k W_{k,i}$ ⇒ $\nabla_x f = 2W^T \cdot q$
        
        Always check: The gradient with respect to a variable should have the same shape as the variable
        
- Summary
    - neural nets will be very large: impractical to write down gradient formula by hand for all parameters
    - in order to get these gradients, backpropagation = recursive application of the chain rule along a computational graph to compute the gradients of all inputs/ parameters/ intermediates
    - implementations maintain a graph structure, where the nodes implement the **forward()**/ **backward()** API
        - forward: compute result of an operation and save any intermediates needed for gradient computation in memory
        - backward: apply the chain rule to compute the gradient of the loss function with respect to the inputs

### Neural Networks

- Functional perspective: without the brain stuff
    
    Before: Linear score function $f = Wx$
    
    Now: 2-layer Neural Network $f = W_2 \max(0, W_1 x)=W_2h$
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2035.png)
    
    $h$ is the value of scores of the templates in $W_1$, and $W_2$ to weight all of the intermediate scores to get final score for the classes
    
    or 3-layer Neural Network $f = W_3 \max(0, W_2 \max(0, W_1 x))$
    

![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2036.png)

- Be very careful with your brain analogies!
    
    Biological Neurons:
    
    - Many different types
    - Dendrites (树突) can perform complex non-linear computations
    - Synapses (突触) are not a single weight but a complex non-inear dynamical system (动态的非线性系统)
    - Rate code may not be adequate
- Neural networks: Architectures
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2037.png)
    
- Summary
    - We arrange neurons into fully-connected layers
    - The abstraction of a **layer** has the nice property that it allows us to use efficient vectorized code (e.g. matrix multiplies)
    - Neural networks are not really neural
- Summary
    
    Process: Mini-batch Stochastic Gradient Descent
    
    Loop:
    
    - **Sample** a batch of data
    - **Forward** prop it through the computational graph (neural network), and get loss
    - **Backprop** to calculate the gradients
    - **Update** the parameters using the gradient

## Lecture 5 Convolutional Neural Networks

[[ConvNetJS demo: training on CIFAR-10]](http://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html)

### Fully Connected Layer

input: 32x32x3 image → stretch to 3072x1

![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2038.png)

Each neuron looks at the full input volume.

Instead of preserving spatial structure, at the last layer at the end, we aggregate all of this together to draw some conclusion.

### Convolutional Layer

Difference: it can preserve spatial structure (空间结构)

- Intro
    
    input: 32x32x3 image → preserve spatial structure
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2039.png)
    
    Convolve the filter (卷积核) with the image, i.e. slide over the image spatially computing dot products
    
    Filters always extend the **depth** of the input volume
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2040.png)
    
    - definition of convolution of two signals
        
        elementwise multiplication and sum of a filter and the signal (image)
        
        $f[x,y]*g[x,y] = \sum\limits_{n_1=-\infty}^{\infty} \sum\limits_{n_2=-\infty}^{\infty} f[n_1,n_2] \cdot g[x-n_1,y-n_2]$
        
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2041.png)
    
    - The brain/neuron view of CONV Layer
        
        An activation map is a 28x28 sheet of neuron
        outputs:
        
        - Each is connected to a small region in the input (rather than looking at the whole input by the Fully Connected Layer)
        - All of them share parameters
        
        a 5x5 filter ⇒ 5x5 receptive field for each neuron
        
    
    consider a second, green filter to learn another specific type of template or concept in the input volumn
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2042.png)
    
    For example, if we had 6 5x5x3 filters, we'll get 6 separate activation maps
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/4843a797-d17d-4fdf-9b1a-c7955ec0f539.png)
    
    We stack these up to get a“new image" of size 28x28x6
    
    - The brain/neuron view of CONV Layer
        
        CONV layer consists of neurons arranged in a 3D grid (28x28x6)
        There will be 6 different neurons all looking at the same region in the input volume looking for different things.
        
    
    Preview: ConvNet is a sequence of Convolutional Layers, interspersed with activation functions
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2043.png)
    
    E.g. 32x32 input convolved repeatedly with 5x5 filters shrinks volumes spatially!
    (32 -> 28 -> 24 ...). Shrinking too fast is not good, doesn’t work well
    
- Aside: Image Processing via Convolutions
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2044.png)
    
- What do convolutional filters learn
    
    ![Each element of these grids is showing what in the input would basically maximizes the activation of the neuron, i.e. what is the neuron looking for.](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2045.png)
    
    Each element of these grids is showing what in the input would basically maximizes the activation of the neuron, i.e. what is the neuron looking for.
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2046.png)
    
- the spatial dimensions of activation map
    
    notation: $F$ = filter size, $N$ = length of input, $P$ = padding, $S$ = stride
    
    - Without padding
    Output size: $(N-F)/S+ 1$
    e.g. $N=7,F=3$:
        - stride 1 ⇒ (7-3)/1+1=5
        - stride 2 ⇒ (7-3)/2+1=3
        - stride 3 ⇒ (7-3)/3+ 1=2.33 don’t fit
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2047.png)
        
    - Common to zero pad the border
    Output size:  $(N+2P-F)/S+ 1$
        - e.g. input 7x7, use 3x3 filter, applied with stride 1, pad with 1 pixel border ⇒ 7x7 output
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2048.png)
        
    - in general, common to see CONV layers with stride 1, filters of size FxF, and zero-padding with $(F-1)/2$
    (will preserve the same size spatially from input to output)
        - F = 3 ⇒ zero pad with 1
        - F = 5 ⇒ zero pad with 2
        - F = 7 ⇒ zero pad with 3
    - example
        
        Input volume: 32x32x3, using 10 5x5 filters with stride 1, pad 2 ⇒ output: 32x32x10
        
        The number of parameters in this layer: 10x(5x5x3+1)=760
        
        (each filter has a bais term)
        
- 1x1 convolution layers also make sense
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2049.png)
    

### Pooling layer

- To makes the representations smaller and more manageable
- operates over each activation map independently
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2050.png)
    
- Commonly use: max pooling
    - No learnable parameters
    - Introduces spatial invariance
    - It’s commen to set up the stride to have them not have any overlap
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2051.png)
    
- spatial dimensions
    
    assume input is $W_1\times H_1\times  C$
    Pooling layer needs 2 hyperparameters:
    
    - The spatial extent $F$
    - The stride $S$
    
    This will produce an output of $W_2 \times H_2\times C$ where:
    
    - $W_2 = (W_1 - F )/S + 1$
    - $H_2 = (H_1 - F)/S + 1$
    
    Number of parameters: 0
    
    Common settings:
    $F=2,S=2$
    $F=3,S=2$
    
- Summary
    - ConvNets stack CONV,POOL,FC layers
    - Trend towards smaller filters and deeper architectures
    - Trend towards getting rid of POOL/FC layers (just CONV)
    - Historically architectures looked like
    $[(\text{CONV}-\text{RELU})^ N-\text{POOL}?]^M-(\text{FC}-\text{RELU})^K,\text{SOFTMAX}$
    where N is usually up to ~5, M is large, 0 ≤ K ≤ 2
    - But recent advances such as ResNet/GoogLeNet have challenged this paradigm

## Lecture 6 Training Neural Networks Part 1

### Activation Functions

![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2052.png)

![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2053.png)

- Sigmoid
    - Squashes numbers to range (0,1)
    - Historically popular since they have nice interpretation as a saturating “firing rate” (放电率) of a neuron
    - Problems
        - Saturated neurons “kill” the gradients (梯度消失)
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2054.png)
            
        - Sigmoid outputs are not zero-centered
            - Consider what happens when the input to a neuron (x) is always positive, what can we say about the gradients on w?
            Always all positive or all negative :(
            (this is also why you want zero-mean data!)
                
                ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2055.png)
                
        - exp() is a bit compute expensive
- tanh
    - Squashes numbers to range [-1,1]
    - zero centered (nice)
    - still kills gradients when saturated :(
- ReLU (Rectified Linear Unit)
    
    Computes $f(x) = max(0,x)$
    
    - Advantages
        - Does not saturate (饱和) (in +region)
        - Very computationally efficient
        - Converges much faster than sigmoid/tanh in practice (e.g. 6x)
        - Actually more biologically plausible than sigmoid
    - Disadvantages
        - Not zero-centered output
        - An annoyance:  what is the gradient when x < 0? ⇒ 0
            - Dead ReLU
                
                ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2056.png)
                
                - 这张图的解释
                    
                    对于不同的权重对应不同的分割超平面，数据经过加权后输入 ReLU，通过超平面将空间分割为0和非0区域，不同的RELU训练得到的分割超平面也相应不同。假设输入数据为二维，这里由于随机初始化w且每个w向量唯一对应二维平面内的一条直线，同时也可代表一个神经元（直线上方的区域表示该神经元的输出为正，下方为负，而为负的会导致回传梯度为0，即神经元无法更新）。当数据云（即所有输入样本构成的集合）全部处于某个神经元对应直线的下方时，将导致无论输入哪个样本都无法使神经元的输出为正，因此该神经元在训练过程中将一直无法更新，所以说dead了
                    
                    people like to initialize ReLU neurons with slightly positive biases (e.g. 0.01) in order to increase the likelihood of it being active at initialization and to get some updates.
                    
        
        Reasons:
        
        - when we have bad initialization
        - when the learning rate is too high
- Leaky ReLU
    
    $f(x) = max(0.01x,x)$
    
    - Advantages:
        - Doesn’t have any saturating (不会饱和)
        - Computationally efficient
        - Converges much faster than sigmoid/tanh in practice! (e.g. 6x)
        - will not “die”.
    
    Parametric Rectifier (PReLU)
    
    $f(x) = max(\alpha x,x)$
    backprop into $\alpha$ (parameter)
    
    more flexible
    
- ELU (Exponential Linear Units)
    - Advantages
        - All benefits of ReLU
        - Closer to zero mean outputs
        - Negative saturation regime compared with Leaky ReLU adds some robustness to noise
    - Disadvantage
        - Computation requires exp()
- Maxout “Neuron”
    
    $\max(w_1^T x + b_1, w_2^T x + b_2)$
    
    - Does not have the basic form of dot product → nonlinearity
    - Advantages
        - Generalizes ReLU and Leaky ReLU
        - Linear Regime! Does not saturate! Does not die!
        - Problem: doubles the number of parameters/neuron :(
- In practice:
    - Use ReLU. Be careful with your learning rates
    - Try out Leaky ReLU / Maxout / ELU
    - Try out tanh but don’t expect much
    - Don’t use sigmoid

### Data Preprocessing

If the input to some layer of the neural network is not centered or normalized, small perturbations (perturbations) in the weight matrix of that layer could cause large perturbations in the output of that layer, which make learning difficult.

- Example
    
    for a binary classification problem
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2057.png)
    

In practice for Images: center only
Not common to normalize variance, to do PCA or whitening

e.g. consider CIFAR-10 example with [32,32,3] images

- Subtract the mean image (e.g. AlexNet) (mean image = [32,32,3] array)
- Subtract per-channel mean (e.g. VGGNet) (mean along each channel = 3 numbers)

![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2058.png)

![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2059.png)

### Weight Initialization

Q: What happens when W=0 init is used?

A: All the neurons will do the same thing → output the same thing, getting the same gradient → update in the same way

- First idea: Small random numbers
    
    W = 0.01 * np.random.randn(D , H) (gaussian with zero mean and 1e-2 standard deviation)
    
    Initialization too small: Activations go to zero, gradients will also be zero → no learning
    
    - Works ~okay for small networks, but problems with deeper networks.
    Q: think about the backward pass. What do the gradients look like?
    Hint: think about backward pass for a W*X gate. → X
    - Example: 10-layer net with 500 neurons on each layer, using tanh non-linearities, and initializing as described above.
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2060.png)
        
- Let weights be all big
    
    Almost all neurons completely saturated (饱和), either -1 and 1. Gradients will be all zero, no learning
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2061.png)
    
- Xavier initialization
    
    Reasonable initialization. (Mathematical derivation assumes linear activations)
    
    W = np.random.randn(fan_in, fan_out) / np.sgrt(fan_in) # layer initialization
    
    激活值的方差是逐层递减的，这导致反向传播中的梯度也逐层递减。要解决梯度消失，就要避免激活值方差的衰减，最理想的情况是，每层的输出值（激活值）保持高斯分布。
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2062.png)
    
    However, when using the ReLU nonlinearity it breaks.
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2063.png)
    

### Batch Normalization

(批量归一化)

- Intro
    
    One way to make deep networks easier to train is to use more sophisticated optimization procedures such as SGD+momentum, RMSProp, or Adam. Another strategy is to change the architecture of the network to make it easier to train. One idea along these lines is batch normalization, proposed by [1] in 2015.
    
    To understand the goal of batch normalization, it is important to first recognize that **machine learning methods tend to perform better with input data consisting of uncorrelated features with zero mean and unit variance**. When training a neural network, we can preprocess the data before feeding it to the network to explicitly decorrelate its features. This will ensure that the first layer of the network sees data that follows a nice distribution. However, even if we preprocess the input data, the activations at deeper layers of the network will likely no longer be decorrelated and will no longer have zero mean or unit variance, since they are output from earlier layers in the network. Even worse, during the training process the distribution of features at each layer of the network will shift as the weights of each layer are updated.
    
    The authors of [1] hypothesize that the shifting distribution of features inside deep neural networks may make training deep networks more difficult. To overcome this problem, they propose to insert into the network layers that normalize batches. At training time, such a layer uses a minibatch of data to estimate the mean and standard deviation of each feature. These estimated means and standard deviations are then used to center and normalize the features of the minibatch. A running average of these means and standard deviations is kept during training, and at test time these running averages are used to center and normalize features.
    
    It is possible that this normalization strategy could reduce the representational power of the network, since it may sometimes be optimal for certain layers to have features that are not zero-mean or unit variance. To this end, the batch normalization layer includes learnable shift and scale parameters for each feature dimension.
    
    [1] [[Sergey Ioffe and Christian Szegedy, "Batch Normalization: Accelerating Deep Network Training by Reducing
    Internal Covariate Shift", ICML 2015.]](https://arxiv.org/abs/1502.03167)
    

consider a batch of activations at some layer. To make each dimension unit gaussian, apply: 

suppose we have $N$ training examples in the current batch and each example has dimension $D$

Steps

- compute the empirical mean and variance independently for each dimension (each feature element).
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2064.png)
    
- Normalize:
$\hat{x}^{(k)} = \frac{x^{(k)} - E[x^{(k)}]}{\sqrt{Var[x^{(k)}]}}$
this is a vanilla differentiable function

Usually inserted **after** Fully Connected or Convolutional layers, and **before** nonlinearity.

![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2065.png)

- Algorithm:
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2066.png)
    
    Note, the network can learn $\gamma(k), \beta(k)$ to get the flexibility to slightly scaled of shifted
    
    - Intuition
        
        我们引入一些 batch normalization 的公式. 这三步就是我们在刚刚一直说的 normalization 工序，但是公式的后面还有一个反向操作, 将 normalize 后的数据再扩展和平移. 原来这是为了让神经网络自己去学着使用和修改这个扩展参数 gamma，和 平移参数 β, 这样神经网络就能自己慢慢琢磨出前面的 normalization 操作到底有没有起到优化的作用，如果没有起到作用，我就使用 gamma 和 belt 来抵消一些 normalization 的操作.
        
        例如在 tanh 激活函数中，我们希望控制饱和的程度，而不是完全不饱和
        
    - gradient
        
        NB 公式： $\hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma^2_B + \varepsilon}},y_i = \gamma \hat{x}_i + \beta$
        
        computational graph:
        
        $$
        \begin{align*}                      &             & \gamma    &             & x_i        & \rightarrow & \rightarrow                     \\                      & \nearrow    &           & \nearrow    &            &             &             & \searrow          \\    L \rightarrow y_i & \rightarrow & \hat{x_i} & \rightarrow & \mu_B      & \rightarrow & \mu_B       & \rightarrow & x_i \\                      & \searrow    &           & \searrow    &            & \nearrow                                      \\
                              &             & \beta     &             & \sigma^2_B &                                               \\\end{align*}
        $$
        
        - 对于缩放参数 $\gamma$ 的梯度：
        $\frac{\partial L}{\partial \gamma} = \sum\limits_{i=1}^{m}\frac{\partial L}{\partial y_i} \frac{\partial y_i}{\partial \gamma} =\sum\limits_{i=1}^{m} \frac{\partial L}{\partial y_i} \cdot \hat{x}_i$
        - 对于偏移参数 $\beta$ 的梯度： 
        $\frac{\partial L}{\partial \beta} = \sum\limits_{i=1}^{m}\frac{\partial L}{\partial y_i} \frac{\partial y_i}{\partial \beta} =\sum\limits_{i=1}^{m} \frac{\partial L}{\partial y_i}$
        - 对于 $\hat{x_i}$ 的梯度 $\frac{\partial L}{\partial \hat{x_i}} = \frac{\partial L}{\partial y_i} \cdot \frac{\partial y_i}{\partial \hat{x}_i}  = \frac{\partial L}{\partial y_i} \cdot \gamma$
        - 对于 $\sigma^2_B$ 的梯度： $\frac{\partial L}{\partial \sigma^2_B} = \sum\limits_{i=1}^{m} \frac{\partial L}{\partial y_i} \cdot \frac{\partial y_i}{\partial \hat{x}_i} \cdot \frac{\partial \hat{x}i}{\partial \sigma^2_B} = \sum\limits_{i=1}^{m} \frac{\partial L}{\partial y_i} \cdot \gamma \cdot \left(x_i - \mu_B\right) \cdot \left[-\frac{1}{2} (\sigma^2_B + \epsilon)^{-\frac{3}{2}}\right]$
        注意是 $\sigma^2_B$ 而不是 $\sigma_B$
        - 对于 $\mu_B$ 的梯度： $\frac{\partial L}{\partial \mu_B} = \sum\limits_{i=1}^{m} \frac{\partial L}{\partial y_i} \cdot \frac{\partial y_i}{\partial \hat{x}_i} \cdot \frac{\partial \hat{x}i}{\partial \mu_B} = \sum\limits_{i=1}^{m} \frac{\partial L}{\partial \hat{x}_i}   \cdot \left(-\frac{1}{\sqrt{\sigma^2_B + \epsilon}}\right) + \frac{\partial L}{\partial \sigma^2_B} \cdot \left(-\frac{2}{m} \sum\limits_{i=1}^{m}(x_i - \mu_B)\right)$
        - 对于 $x_i$ 的梯度： $\frac{\partial L}{\partial x_i} = \frac{\partial L}{\partial \hat{x}_i} \cdot \frac{1}{\sqrt{\sigma^2_B + \varepsilon}} + \frac{\partial L}{\partial \sigma^2_B} \cdot \frac{2}{m}(x_i - \mu_B) + \frac{\partial L}{\partial \mu_B} \cdot \frac{1}{m}$
- Advantages
    - Improves gradient flow through the network
    - Allows higher learning rates
    - Reduces the strong dependence on initialization
    - Acts as a form of regularization in a funny way, and slightly reduces the need for dropout, maybe
- Note:
    
    at **test time** BatchNorm layer functions differently:
    The mean/std are not computed based on the batch. Instead, a single fixed empirical mean of activations during training is used. (e.g. can be estimated during training with running averages)
    
    将在训练集中计算得到的均值方差应用到之后所有的数据中，包括测试集、验证集，目的是给予所有数据相同的标准化力度
    
- Layer Normalization
    
    Batch normalization has proved to be effective in making networks easier to train, but the dependency on batch size makes it less useful in complex networks which have a cap on the input batch size due to hardware limitations. 
    
    Several alternatives to batch normalization have been proposed to mitigate this problem; one such technique is Layer Normalization [2]. Instead of normalizing over the batch, we normalize over the features. In other words, when using Layer Normalization, each feature vector corresponding to a single datapoint is normalized based on the sum of all terms within that feature vector.
    
    [2] [Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. "Layer Normalization." stat 1050 (2016): 21.](https://arxiv.org/pdf/1607.06450.pdf)
    
- Spatial Batch Normalization
    
    We already saw that batch normalization is a very useful technique for training deep fully connected networks. As proposed in the original paper (link in `BatchNormalization.ipynb`), batch normalization can also be used for convolutional networks, but we need to tweak it a bit; the modification will be called "spatial batch normalization."
    
    Normally, batch-normalization accepts inputs of shape `(N, D)` and produces outputs of shape `(N, D)`, where we normalize across the minibatch dimension `N`. For data coming from convolutional layers, batch normalization needs to accept inputs of shape `(N, C, H, W)` and produce outputs of shape `(N, C, H, W)` where the `N` dimension gives the minibatch size and the `(H, W)` dimensions give the spatial size of the feature map.
    
    If the feature map was produced using convolutions, then **we expect every feature channel's statistics e.g. mean, variance to be relatively consistent both between different images, and different locations within the same image** -- after all, every feature channel is produced by the same convolutional filter! Therefore, spatial batch normalization computes a mean and variance for each of the `C` feature channels by computing statistics over the minibatch dimension `N` as well the spatial dimensions `H` and `W`.
    
    [1] [Sergey Ioffe and Christian Szegedy, "Batch Normalization: Accelerating Deep Network Training by Reducing
    Internal Covariate Shift", ICML 2015.](https://arxiv.org/abs/1502.03167)
    

### Babysitting the Learning Process

(观察学习过程)

- Double check that the loss is reasonable:
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2067.png)
    
- Try to train
    - Make sure that you can overfit very small portion of the training data
        - take the first 20 examples from CIFAR-10
        - turn off regularization (reg = 0.0)
        - use simple vanilla ‘sgd’
    - Start with small regularization and find learning rate that makes the loss go down.
        - Loss barely changing: Learning rate is probably too low
        - cost: NaN almost always means high learning rate…
        - Rough range for learning rate we should be cross-validating is [1e-3 … 1e-5]

### Hyperparameter Optimization

- Hyperparameters:
    - network architecture
    - learning rate, its decay schedule, update type
    - regularization (L2/Dropout strength)
- Cross-validation strategy
    
    coarse -> fine cross-validation in stages
    
    - First stage: only a few epochs to get rough idea of what params work
    - Second stage: longer running time, finer search
    - … (repeat as necessary)
    
    Tip for detecting explosions in the solver: If the cost is ever > 3 * original cost, break out early
    
    - example
        - run coarse search  for 5 epochs
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2068.png)
            
        - Now run finer search
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2069.png)
            
- Random Search vs. Grid Search
    
    随机取值可以在每个维度覆盖更多的值
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2070.png)
    
- Monitor and visualize the loss curve
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2071.png)
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2072.png)
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2073.png)
    

## Lecture 7 Training Neural Networks Part 2

### Parameter update schemes

- Problems with SGD
    
    SGD:
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2074.png)
    
    - One problem: poorly conditioned coordinate
        
        What if loss changes quickly in one direction and slowly in another? What does gradient descent do?
        Very slow progress along shallow dimension, jitter along steep direction
        
        Because the direction of the gradient does not align with the direction towards the minima 
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2075.png)
        
        It’s much more common in high dimension that the ratio is very large 
        
    - Another problem: local minima and saddle points
        
        What if the loss function has a local minima or saddle point (鞍点)?
        ⇒ Zero gradient, gradient descent gets stuck
        
        Saddle points much more common in high dimension
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2076.png)
        
    - Another problem: ‘S’
        
        Our gradients come from mini-batches so they can be noisy!
        
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2077.png)
    
- SGD + Momentum
    - Build up “**velocity**” as a running mean of gradients
    - hyperparameter $\rho$  gives “friction”, typically $\rho$ = 0.9 or 0.99
    - We step in the direction of our velocity vector rather than the direction of the raw gradient vector
    - Intuitively, the velocity is kind of a weighted sum of the gradients over time, with more recent gradients being weighted heavier
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2078.png)
        
        result:
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2079.png)
        
    - Nesterov Momentum
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2080.png)
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2081.png)
        
        result:
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2082.png)
        
        because of the correction factor in Nesterov, it’s not overshooting (超调) quite as drastically compared to vanilla momentum
        
- AdaGrad
    
    Added element-wise scaling of the gradient based on the historical sum of squares in each dimension
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2083.png)
    
    1e-7: to make sure we’re not dividing by 0
    
    - Q1: What happens with AdaGrad?
        
        A: Suppose we have two coordinates, one that always has a very high gradient and one that always has a verysmall gradient
        
        when we add the sum of the squares of the small gradient, we're going to be dividing by a small number, so we'l accelerate movement along the slow dimension
        
        along the other dimension, where the gradients tend to be very large, so we’ll slow down the progress along the wiggling dimension
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2084.png)
        
    - Q2: What happens to the step size over long time?
        
        in the convex case, there's some really nice theory showing that this is actually really good case in the convex case, as you approach a minimum, you kind of want to slow down so you actually converge
        
        But in the non-convex case, that's a little bit problematic because as you come towards a saddle point, you might get stuck with AdaGrad, and then you kind of no longer make any progress.
        
    - A variation of AdaGrad: RMSProp
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2085.png)
        
        decay_rate: commonly 0.9 or 0.99
        
        result:
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2086.png)
        
- Adam
    - approximately
        
        Adam is sort of RMSProp with momentum
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2087.png)
        
        Q: What happens at first timestep?
        
        A: second_moment is very close to 0, which will make a very large step at the beginning
        
    - full form
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2088.png)
        
        we create unbiased estimate (无偏估计) of the two moments by incorporating the current time step $t$, and we update use these unbiased estimate rather than the original moments.
        
        Adam with $\beta_1 = 0.9$, $\beta_2 = 0.999$, and learning_rate = 1e-3 or 5e-4 is a great starting point for many models!
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2089.png)
        

### Learning rate schedules

SGD, SGD+Momentum, Adagrad, RMSProp, Adam all have **learning rate** as a hyperparameter.

Let Learning rate decay over time!

- step decay:
e.g. decay learning rate by half every few epochs.
- exponential decay (指数衰减):
$\alpha = \alpha_0 e^{-kt}$
- 1/t decay:
$\alpha = \frac{\alpha_0}{(1 + kt)}$

![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2090.png)

imagine that the model got near some good region and the gradients is small, so it’s kind of bouncing around too much. If we drop the learning rate, it lets it continue to make progress

Learning rate decay is kind of a second-order hyperparameter, shouldn’t be optimized from the start. We should pick a good learning rate with no learning rate decay at the beginning, and trying to cross-validate jointly over learning rate decay and initial learning rate.

### Second-Order Optimization

- First-Order Optimization
    
    上述我们讲的都属于这种
    
    求一阶微分，用直线近似 Loss 曲线，并朝前迈一小步
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2091.png)
    
- Second-Order Optimization:
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2092.png)
    
    second-order Taylor expansion (高维情况下对目标函数 $J(\theta)$ 进行二阶泰勒展开):
    
    $J(\theta) \approx J(\theta_0) + (\theta - \theta_0)^T\nabla_{\theta}J(\theta_0) + \frac{1}{2}(\theta - \theta_0)^TH(\theta - \theta_0)$
    
    Solving for the critical point we obtain the Newton parameter update:
    
    $\theta^* = \theta_0 - H^{-1}\nabla_{\theta}J(\theta_0)$
    
    - Hessian Matrix
        
        $H$ 代表海森矩阵（Hessian Matrix），它是目标函数 $J(\theta)$ 关于参数 $\theta$ 的二阶偏导数组成的矩阵。具体地，对于一个多元函数 $J(\theta_1, \theta_2, ..., \theta_n)$，其海森矩阵 $H$ 定义为：$H_{ij} = \frac{\partial^2 }{\partial \theta_i \partial \theta_j}J(\theta)$
        这里的 $i$ 和 $j$ 分别表示参数的下标，取值范围从 1 到 $n$。每个元素 $H_{ij}$ 表示函数在点 $\theta$ 处关于第 $i$ 个参数和第 $j$ 个参数的混合偏导数。
        
    
    step directly to the minimum of this quadratic approximationn (二次逼近) to tour function → **No Learning rate**
    
    - 缺点：计算量太大
        
        Hessian has $O(N^2)$ elements, Inverting takes $O(N^3)$
        
    - 改进：
        - Quasi-Newton methods (BGFS most popular):
        instead of inverting the Hessian (O(n^3)), approximate
        inverse Hessian with rank 1 updates over time ($O(n^2)$
        each).
        - L-BFGS (Limited memory BFGS):
        Does not form/store the full inverse Hessian.
    - In practice
        - **Adam** is a good default choice in most cases
        - If you can afford to do full batch updates then try out L-BFGS (and don’t forget to disable all sources of noise)

### Model Ensembles

(模型融合)

- One way
    - Train multiple independent models (can have different hyperparameter)
    - At test time average their results
    - Enjoy 2% extra performance
- Another way
    - Instead of training independent models, use multiple **snapshots** of a single model during training!
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2093.png)
    
- Another way
    - Instead of using actual parameter vector, keep a moving average of the parameter vector and use that at test time (Polyak averaging)
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2094.png)
    

### Regularization

(正则化)

How to improve single-model performance? Use **regularization**

- Dropout
    
    In each forward pass, randomly set some activations of neurons to zero
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2095.png)
    
    The probability of dropping is a hyperparameter, 0.5 is common
    
    Commonly used in fully connected layer
    
    - Implement
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2096.png)
        
    - interpretation
        - Forces the network to have a redundant representation (冗余), to distribute its idea across different features
        Prevents co-adaptation (相互适应) of features to help prevent overfitting
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2097.png)
            
        - Dropout is like training a large ensemble of models (that share parameters) within a single model.
            - Each binary mask is one model
    - Dropout at Test time
        
         Dropout makes our output random,so we want to “**average out**” the randomness at test-time
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2098.png)
        
        $y = f(x) = E_z \left[ f(x, z) \right] = \int p(z) f(x, z) dz$
        
        - approximate the integral
            
            Consider a single neuron:
            
            - At test time we have: $E[a] = w_1 x + w_2 y$
            - During training we have: $E[a] = \frac{1}{4}(w_1 x + w_2 y) + \frac{1}{4}(w_1 x + 0 y) + \frac{1}{4}(0 x + 0 y) + \frac{1}{4}(0 x + w_2 y) = \frac{1}{2}(w_1 x + w_2 y)$
            
            **At test time, multiply by dropout probability**
            
    - Summary
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2099.png)
        
        More common: “Inverted dropout”
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20100.png)
        
- More general strategy
    
    Strategy
    
    - Training: Add some kind of randomness to the network to prevent from over-fitting 
    $y = f_W(x, z)$
    - Testing: Average out randomness (sometimes approximate) to improve generalization
    $y = f(x) = E_z \left[ f(x, z) \right] = \int p(z) f(x, z) dz$
    - Example
        - Dropout
        - Batch Normalization
            - Training: Normalize using stats from random minibatches
            - Testing: Use fixed stats to normalize
        - Data Augmentation
            - Random mix/combinations of :
                - translation
                - rotation
                - stretching
                - shearing
                - lens distortions
                - …
        - DropConnect
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20101.png)
            
        - Fractional Max Pooling (部分最大池化)
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20102.png)
            
        - Stochastic Depth
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20103.png)
            

### Transfer learning / fine-tuning

Somtime we overfit just because we don’t have enough data, transfer learning is another way to combat that (one way is regularization).

![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20104.png)

![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20105.png)

Transfer learning with CNNs is pervasive (it’s the norm, not an exception)

![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20106.png)

Takeaway for your projects and beyond:
Have some dataset of interest but it has < ~1M images?

- Find a very large dataset that has similar data, train a big ConvNet there
- or transfer learn to your dataset
Deep learning frameworks provide a “Model Zoo” of pretrained models so you don’t need to train your own
Caffe: [https://github.com/BVLC/caffe/wiki/Model-Zoo](https://github.com/BVLC/caffe/wiki/Model-Zoo)
TensorFlow: [https://github.com/tensorflow/models](https://github.com/tensorflow/models)
PyTorch: [https://github.com/pytorch/vision](https://github.com/pytorch/vision)

## Lecture 8  Deep Learning Software

### CPU vs GPU

- CPU: Fewer cores, but each core is much faster and much more capable; great at sequential tasks
- GPU: More cores, but each core is much slower and “dumber”; great for parallel tasks
- Example: Matrix Multiplication
    
    for massively parallel problems, GPUs do much better than CPUs
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20107.png)
    
- Example: Convolution
    
    we have an input tensor and weight tensor, at every point in the output tensor after a convolution is again some inner produet between some part of the weights and some part of the input.
    
    A GPU could paralyze this conputation, split it all up across the many cores and compute it very quickly.
    
- Programming GPUs
    - CUDA (NVIDIA only)
        - Write C-like code that runs directly on the GPU
        - Higher-level APIs: cuBLAS, cuFFT, cuDNN, etc
    - OpenCL
        - Similar to CUDA, but runs on anything
        - Usually slower :(
    - Udacity: Intro to Parallel Programming
    [https://www.udacity.com/course/cs344](https://www.udacity.com/course/cs344)
    - For deep learning just use existing libraries, we don’t need to write CUDA code from scratch by ourselves
- CPU / GPU Communication
    
    If you aren’t careful, training can bottleneck (瓶颈) on reading data and transferring to GPU!
    Solutions:
    
    - Read all data into RAM
    - Use SSD instead of HDD
    - Use multiple CPU threads to prefetch data

### Deep Learning Frameworks

![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20108.png)

- The point of deep learning frameworks
    - Easily build big computational graphs
    - Easily compute gradients in computational graphs
    - Run it all efficiently on GPU (wrap cuDNN, cuBLAS, etc)
- Example
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20109.png)
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20110.png)
    
    - Problems when written in Numpy:
        - Can’t run on GPU, Numpy is CPU only
        - Have to compute our own gradients
    
    In the forward pass, the TensorFlow and PyTorch code look almost like Numpy because Numpy has a beautiful API. 
    
    But we can compute gradients automatically and we can run on the GPU by using TensorFlow or PyTorch
    

### TensorFlow

[TensorFlow.pdf](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/TensorFlow.pdf)

### Pytorch

[PyTorch.pdf](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/PyTorch.pdf)

### **Static vs Dynamic Graphs**

[Static vs Dynamic Graphs.pdf](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Static_vs_Dynamic_Graphs.pdf)

### Caffe

[Caffe.pdf](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Caffe.pdf)

- **Advice**
    - **TensorFlow** is a safe bet for most projects. Not perfect but has huge community, wide usage. Maybe pair with high-level wrapper (Keras, Sonnet, etc)
    - I think **PyTorch** is best for research. However still new, there can be rough patches.
    - Use **TensorFlow** for one graph over many machines
    - Consider **Caffe**, **Caffe2,** or **TensorFlow** for production deployment
    - Consider **TensorFlow** or **Caffe2** for mobile

## Lecture 9 CNN Architectures

[Lecture 9 CNN Architectures.pdf](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Lecture_9_CNN_Architectures.pdf)

## Lecture 10 Recurrent Neural Nwetworks

### Intro

- Recurrent Neural Networks: Process Sequences
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20111.png)
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20112.png)
    
    Notice: the same function and the same set of parameters $W$ are used at every time step.
    
    - (Vanilla) Recurrent Neural Network
        
        The state consists of a single “hidden” vector $h$
        
        $h_t = f_W(h_{t-1}, x_t)$ →
        $h_t = \tanh(W_h h_{t-1} + W_x x_t+b)$
        $y_t = W_y h_t$
        
        - $tanh$ 的导数
            
            $\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$
            
            $\frac{d}{dx} \tanh(x) = \frac{\left(e^x + e^{-x}\right)\left(e^x + e^{-x}\right) - \left(e^x - e^{-x}\right)\left(e^x - e^{-x}\right)}{\left(e^x + e^{-x}\right)^2}
            = 1 - \frac{\left(e^x - e^{-x}\right)^2}{\left(e^x + e^{-x}\right)^2} = 1 - \tanh^2(x)$
            
    - Computational Graph
        - Many to Many
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20113.png)
            
        - Many to One
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20114.png)
            
        - One to Many
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20115.png)
            
        - Sequence to Sequence: Many-to-one + one-to-many
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20116.png)
            
    - Example: Character-level Language Model
        
        Vocabulary: [h,e,l,o] Example training sequence: “hello”
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20117.png)
        
        At test-time sample characters one at a time, feed back to model
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20118.png)
        
        Q: Why sample instead of just taking the character with the largest score?
        
        A: Can get more diversity in the outputs.
        
    - backprop
        - Backpropagation through time
            - Forward through entire sequence to compute loss, then backward through entire sequence to compute gradient
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20119.png)
            
        - Truncated Backpropagation through time
            - Run forward and backward through chunks of the sequence instead of whole sequence
            - Carry hidden states forward in time forever, but only backpropagate for some smaller number of steps
            - Is a way to approximate the gradients without going making a backwards pass through your potentially very large sequence of data
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20120.png)
            
    - Python implement
        
        [https://gist.github.com/karpathy/d4dee566867f8291f086](https://gist.github.com/karpathy/d4dee566867f8291f086)
        
    - Multilayer RNNs
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20121.png)
        
    - Vanilla RNN Gradient Flow
        
        $$
        \begin{align*}
        h_t &= \tanh(W_{hh} h_{t-1} + W_{xh} x_t) \\
        &= \tanh \left( \begin{pmatrix} W_{hh} & W_{hx}\end{pmatrix} \begin{pmatrix} h_{t-1} \\ x_t \end{pmatrix} \right) \\
        &= \tanh \left( W \begin{pmatrix} h_{t-1} \\ x_t \end{pmatrix} \right)
        \end{align*}
        $$
        
        Backpropagation from $h_t$ to $h_{t-1}$ multiplies by $W$ (actually $W^T_{hh}$)
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20122.png)
        
        Computing gradient of $h_0$ involves many factors of $W$ (and repeated tanh)
        
        - Largest singular value of $W$ > 1: Exploding gradients
        - Largest singular value of $W$ < 1: Vanishing gradients
        
        理解：比如 $W$ 是一个数，如果 $|W|>1$，则 $W^n\to \infty$，如果 $|W|<1$，则 $W^n\to 0$
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20123.png)
        
        - Gradient clipping: Scale gradient if its norm is too big
            - To cope with exploding gradients
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20124.png)
            
        - Change RNN architecture
            - To cope with vanishing gradients
    - Long Short Term Memory (LSTM)
        
        Designed to help alleviate the problem of vanishing and exploding gradients
        
        - LSTM has two hidden states at every time step
            - $h_t$ the hidden state, will reveal to the outside world
            - $c_t$ the cell state
        - four gates
            - f: Forget gate, Whether to erase cell
            - i: Input gate, whether to write to cell
            - g: Gate gate (?), How much to write to cell
            - o: Output gate, How much to reveal cell
        
        $$
        \begin{align*}
        \begin{pmatrix}
        i \\
        f \\
        o \\
        g
        \end{pmatrix}
        &=
        \begin{pmatrix}
        \sigma \\
        \sigma \\
        \sigma \\
        \tanh
        \end{pmatrix}
        W
        \begin{pmatrix}
        h_{t-1} \\
        x_t
        \end{pmatrix}
        \\
        c_t& = f \odot c_{t-1} + i \odot g
        \\
        h_t& = o \odot \tanh(c_t)
        \end{align*}
        $$
        
        $\odot$ means the element wise product of two matrixs
        
        - Backprop
            
            Backpropagation from $c_t$ to $c_{t-1}$ only **elementwise
            multiplication** by $f$, no matrix multiply by $W$
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20125.png)
            
- Summary
    - RNNs allow a lot of flexibility in architecture design
    - Vanilla RNNs are simple but don’t work very well
    - Common to use LSTM or GRU: their additive interactions
    improve gradient flow
    - Backward flow of gradients in RNN can explode or vanish.
    Exploding is controlled with gradient clipping. Vanishing is
    controlled with additive interactions (LSTM)
    - Better/simpler architectures are a hot topic of current research
    - Better understanding (both theoretical and empirical) is needed.

## Lecture 11 Detection and Segmentation

### Image Classification

![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20126.png)

### Other Computer Vision Tasks

![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20127.png)

- Semantic Segmentation
    
    Label each pixel in the image with a category label
    Don’t differentiate instances (区分实例), only care about pixels
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20128.png)
    
    - Semantic Segmentation Idea
        - Sliding Window
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20129.png)
            
            Problem: Very inefficient! Not reusing shared features between overlapping patches
            
        - Fully Convolutional
            
            Design a network as a bunch of convolutional layers to make predictions for pixels all at once!
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20130.png)
            
            Design network as a bunch of convolutional layers, with downsampling and upsampling inside the network!
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20131.png)
            
            Downsampling: Pooling, strided convolution
            
            - Strided convolution (跨步卷积)
                
                3 x 3 convolution, stride 2 pad 1
                
                ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20132.png)
                
                Filter moves 2 pixels in the input for every one pixel in the output
                
                Stride gives ratio between movement in input and
                output
                
            - In-Network upsampling: “Unpooling”
                
                ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20133.png)
                
                - Max Unpooling
                    
                    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20134.png)
                    
                - Tranpose Convolution
                    
                    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20135.png)
                    
                    Other names: 
                    
                    Deconvolution (bad), Upconvolution, Fractionally strided convolution, Backward strided convolution
                    
                    1D Example
                    
                    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20136.png)
                    
                    Convolution as Matrix Multiplication (1D Example)
                    
                    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20137.png)
                    
                    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20138.png)
                    
- Classification + Localization
    
    You have some **fix number of objects** that you are looking for
    
    Treat localization as a regression problem.
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20139.png)
    
    Aside: Human Pose Estimation
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20140.png)
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20141.png)
    
- Object Detection
    
    we don’t know how many object instances we’re looking for in the image ahead of time
    
    - Object Detection as Regression? X
    Each image needs a different number of outputs!
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20142.png)
        
    - Object Detection as Classification: Sliding Window
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20143.png)
        
        Problem: Need to apply CNN to huge number of locations and scales, very computationally expensive!
        
    - Region Proposals (候选区域)
        - Find “blobby” image regions that are likely to contain objects
        - Relatively fast to run; e.g. Selective Search gives 1000 region proposals in a few seconds on CPU
        - very high recall
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20144.png)
        
        - R-CNN
            
            Region-based Convolutional Neural Networks
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20145.png)
            
            - Ad hoc training objectives
                - Fine-tune network with softmax classifier (log loss)
                - Train post-hoc linear SVMs (hinge loss)
                - Train post-hoc bounding-box regressions (least squares)
                - Training is slow (84h), takes a lot of disk space
                - Inference (detection) is slow
                - 47s / image with VGG16 [Simonyan & Zisserman. ICLR15]
                - Fixed by SPP-net [He et al. ECCV14]
        - Fast R-CNN
            
            Taking crops from the convolutional feature map corresponding to each proposal rather than taking crops directly from the image → reuse a lot of expensive convolutional computation across the entire image
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20146.png)
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20147.png)
            
            Problem: Runtime dominated by region proposals! Computing the region proposals using the fixed function became a bottleneck.
            
        - Faster R-CNN
            
            Make CNN itself do proposals!
            Insert Region Proposal Network (RPN) to predict proposals from features
            
            - Jointly train with 4 losses:
                - RPN classify object / not object
                - RPN regress box coordinates
                - Final classification score (object classes)
                - Final box coordinates
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20148.png)
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20149.png)
            
    - Detection without Proposals: YOLO / SSD
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20150.png)
        
        Go from input image to tensor of scores with one big convolutional network!
        
        - Within each grid cell:
            - Regress from each of the B base boxes to a final box with 5 numbers: (dx, dy, dh, dw, confidence)
            - Predict scores for each of C classes (including background as a class)
        
        Output: 7 x 7 x (5 * B + C)
        
    - Faster R-CNN is slower but more accurate,
    - SSD is much faster but not as accurate
    - Aside: Object Detection + Captioning = Dense Captioning
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20151.png)
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20152.png)
        
- Instance Segmentation
    
    a hybrid between Semantic Segmentation and Object Detection
    
    - Mask R-CNN
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20153.png)
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20154.png)
        
    - Mask R-CNN Also does pose
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20155.png)
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20156.png)
        

## Lecture 12 Visualizing and Understanding

[Lecture 12 Visualizing and Understanding.pdf](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Lecture_12_Visualizing_and_Understanding.pdf)

## Lecture 13 Generative Models

### Unsupervised Learning

- Data: x Just data, no labels!
→ Training data is cheap
- Goal: Learn some underlying hidden structure of the data
Holy grail: Solve unsupervised learning → understand structure of visual world
- Examples: Clustering, dimensionality reduction, feature learning, density estimation, etc.
- Generative Models
    
    Given training data, generate new samples from same distribution
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20157.png)
    
    Want to learn $p_{model}(x)$ similar to $p_{data}(x)$
    
    Addresses density estimation, a core problem in unsupervised learning
    Several flavors
    
    - Explicit density estimation: explicitly define and solve for $p_{model}(x)$
    - Implicit density estimation: learn model that can sample from $p_{model}(x)$ w/o explicitly defining it
    - Why Generative Models?
        - Realistic samples for artwork, super-resolution, colorization, etc.
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20158.png)
            
        - Generative models of time-series data can be used for simulation and planning (reinforcement learning applications!)
        - Training generative models can also enable inference of  latent representations that can be useful as general features
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20159.png)
    

### PixelRNN and PixeICNN

- Fully visible belief network
    
    Explicit density model
    Use chain rule to decompose likelihood of an image x into product of 1-d distributions:
    
    $p(x) = \prod\limits_{i=1}^{n} p(x_i | x_1, ..., x_{i-1})$
    
    $p(x)$ is the likelihood of image x
    
    $p(x_i | x_1, ..., x_{i-1})$ is the probability of i’th pixel value given all previous pixels
    
    Then maximize likelihood of training data
    
    - 全显式信念网络
        
        这是一张关于全显式信念网络（Fully visible belief network）的图片，它是一种概率模型。在这个网络中，使用链式法则将图像x的概率分解为一系列单维分布的乘积。
        
        具体来说，该图展示了如何通过以下步骤来构建一个全显式信念网络：
        
        1. 使用链式法则将图像x的概率p(x)表示为每个像素值xi的概率与所有先前像素值的条件概率的乘积。
        2. 定义“前一像素”的顺序，以便计算给定所有先前像素的i'th像素值的概率。
        3. 将复杂的像素值分布表达为神经网络的形式，以实现对训练数据最大似然性的优化。
        
        最后，通过最大化训练数据的似然性来学习网络参数，从而得到一个能够描述图像x的概率分布的模型。这个过程通常用于生成式建模任务，如图像生成或语音合成等。
        
- PixelRNN
    
    Generate image pixels starting from corner
    
    Dependency on previous pixels modeled using an RNN (LSTM)
    
    Drawback: sequential generation is slow!
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20160.png)
    
- PixelCNN
    
    Still generate image pixels starting from corner
    
    Dependency on previous pixels now modeled using a CNN over context region
    
    - Training: maximize likelihood of training images
        
        在机器学习和统计学中，“最大似然”是一种估计模型参数的方法。
        
        其基本思想是：选择使得观察数据出现的可能性最大的参数值。简单来说，就是找到一组参数，使得我们用这些参数构建的模型预测出已知观测结果的概率最高。 
        
        对于 PixelCNN 这样的生成模型，目标通常是训练一个能够很好地模拟真实数据分布的模型。通过最大化训练集上每个样本的联合概率，我们可以得到一个更接近于真实数据分布的模型。这样，在生成新样本时，模型可以产生与训练数据相似的结果。 PixelCNN 使用了条件神经网络来建模每个像素值与其他像素之间的依赖关系，并且通过 Softmax 损失函数进行优化。
        
        Softmax 函数将输出映射到一个概率分布，使得所有的输出值都在 [0, 1] 范围内，并且它们的总和为 1。因此，Softmax 损失函数通常用于多分类问题，以最小化预测概率分布与实际标签分布之间的差距。
        
         在 PixelCNN 的训练过程中，模型会尝试调整参数，使得对每一个训练样本，模型都能给出较高的似然度（即较大的预测概率）。
        
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20161.png)
    
    Training is faster than PixelRNN (can parallelize convolutions since context region values known from training images)
    Generation must still proceed sequentially → still slow
    
- recap
    - Pros
        - Can explicitly compute likelihood $p(x)$, which is an explicit density that we can optimize.
        - Explicit likelihood of training data gives good evaluation metric
        - Produce good samples
    - Con
        - Sequential generation → slow

### Variational Autoencoders (VAE)

(变分自动编码器)

- Intro
    
    PixelCNNs define tractable density function, optimize likelihood of training data:
    
    $p_{\theta}(x) = \prod_{i=1}^{n} p_{\theta}(x_i | x_1, ..., x_{i-1})$
    
     VAEs define intractable density function with latent $z$: 
    
    $p_{\theta}(x) = \int p_{\theta}(z)p_{\theta}(x|z)dz$
    
    Cannot optimize directly, derive and optimize lower bound on likelihood instead
    
- Autoencoders
    
    Autoencoders is an unsupervised approach for learning a lower-dimensional feature representation from unlabeled training data
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20162.png)
    
    Q: Why dimensionality reduction from $z$ to $x$? 
    
    A: Want features to capture meaningful factors of variation in data
    
    - How to learn this feature representation?
        - Train such that features can be used to reconstruct original data “Autoencoding” - encoding itself
        - After training, throw away decoder
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20163.png)
        
- Variational Autoencoders
    
    Probabilistic spin on autoencoders → will let us sample from the model to generate data!
    
    Assume training data $\left\{\mathbf{x}^{(i)}\right\}_{i=1}^N$ is generated from underlying unobserved (latent) representation $z$
    
    We want to estimate the true parameters of this generative model.
    
    - How should we represent this model?
        - Choose prior $p(z)$ to be simple, e.g. Gaussian. Reasonable for latent attributes, e.g. pose, how much smile.
        - Conditional $p(x|z)$ is complex (generates image) → represent with neural network
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20164.png)
    
    How to train the model?
    Remember strategy for training generative models from FVBNs. Learn model parameters to maximize likelihood of training data $p_{\theta}(x) = \int p_{\theta}(z)p_{\theta}(x|z)dz$
    
    Q: What is the problem with this?
    A: Intractable!
    
    - Intractability
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20165.png)
        
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20166.png)
    
    Now equipped with our encoder and decoder networks, let’s work out the (log) data likelihood:
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20167.png)
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20168.png)
    
    Generating Data
    
    Use decoder network.  Now sample z from prior
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20169.png)
    
- recap
    
    Probabilistic spin to traditional autoencoders → allows generating data
    Defines an intractable density → derive and optimize a (variational) lower bound
    
    - Pros:
        - Principled approach to generative models
        - Allows inference of q(z|x), can be useful feature representation for other tasks
    - Cons:
        - Maximizes lower bound of likelihood: okay, but not as good evaluation as PixelRNN/PixelCNN
        - Samples blurrier and lower quality compared to state-of-the-art (GANs)
    - Active areas of research:
    - More flexible approximations, e.g. richer approximate posterior instead of diagonal Gaussian
    - Incorporating structure in latent variables

### Generative Adversarial Networks(GAN)

GANs: don’t work with any explicit density function! Instead, take game-theoretic (博弈论) approach: learn to generate from training distribution through 2-player game

- Problem: Want to sample from complex, high-dimensional training distribution.  No direct way to do this!
- Solution: Sample from a simple distribution, e.g. random noise.
Learn transformation to training distribution.

Q: What can we use to represent this complex transformation?
A: A neural network!

![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20170.png)

- Training GANs: Two-player game
    - Generator network (生成网络) $G(z;\theta_{g})$: try to fool the discriminator by generating real-looking images
    - Discriminator network (判别网络) $D(x;\theta_{d})$: try to distinguish between real and fake images
    - Train jointly in minimax game
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20171.png)
    
    Minimax objective function:
    
    $\min\limits_{\theta_g}\max\limits_{\theta_d}\left[\mathbb{E}_{x\sim p_{data}}\log D_{\theta_d}(x)+\mathbb{E}_{z\sim p(z)}\log\left(1-D_{\theta_d}(G_{\theta_g}(z))\right)\right]$
    
    - $D_{\theta_d}(x)$: The discriminator output for real data $x$
    - $D_{\theta_d}(G_{\theta_g}(z))$: The discriminator output for generated fake data $G(z)$
    - Discriminator ($\theta_d$) wants to maximize objective such that $D(x)$ is close to 1 (real) and $D(G(z))$ is close to 0 (fake)
    - Generator ($\theta_g$) wants to minimize objective such that $D(G(z))$ is close to 1 (discriminator is fooled into thinking generated $G(z)$ is real)
    
    Alternate between
    
    - Gradient ascent on discriminator
    $\max\limits_{\theta_d}\left[\mathbb{E}_{x\sim p_{data}}\log D_{\theta_d}(x)+\mathbb{E}_{z\sim p(z)}\log\left(1-D_{\theta_d}(G_{\theta_g}(z))\right)\right]$
    - Gradient ascent on generator, different objective
    $\max\limits_{\theta_g}\mathbb{E}_{z\sim p(z)}\log\left(D_{\theta_d}(G_{\theta_g}(z))\right)$
    
    Instead of minimizing likelihood of discriminator being correct, now maximize likelihood of discriminator being wrong.
    Same objective of fooling discriminator, but now higher gradient
    signal for bad samples => works much better! Standard in practice.
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20172.png)
    
    Aside: Jointly training two networks is challenging, can be unstable.  Choosing objectives with better loss landscapes helps training, is an active area of research.
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20173.png)
    
    - Putting it together: GAN training algorithm
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20174.png)
        
    
    After training, use generator network to generate new images
    
- Recap
    
    Don’t work with an explicit density function
    Take game-theoretic approach: learn to generate from training distribution through 2-player game
    
    - Pros:
        - Beautiful, state-of-the-art samples!
    - Cons:
        - Trickier / more unstable to train
        - Can’t solve inference queries (推理查询) such as p(x), p(z|x)
    - Active areas of research:
        - Better loss functions, more stable training (Wasserstein GAN, LSGAN, many others)
        - Conditional GANs, GANs for all kinds of applications
- Recap
    
    Generative Models
    
    - PixelRNN and PixelCNN
    Explicit density model, optimizes exact likelihood, good
    samples. But inefficient because sequential generation.
    - Variational Autoencoders (VAE)
    Optimize variational lower bound on likelihood. Useful
    latent representation, inference queries (推理查询). But current sample quality not the best.
    - Generative Adversarial Networks (GANs)
    Game-theoretic approach, best samples! But can be tricky and unstable to train, no inference queries.

## Lecture 14 Reinforcement Learning

### Reinforcement Learning

Problems involving an agent interacting with an environment,
which provides numeric reward signals
Goal: Learn how to take actions in order to maximize reward

![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20175.png)

- Example
    
    Cart-Pole Problem
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20176.png)
    
    Robot Locomotion
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20177.png)
    
    Atari Games
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20178.png)
    
    Go
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20179.png)
    

### Markov Decision Processes

- Mathematical formulation of the RL problem
- Markov property: Current state completely characterises the state of the
world
- Defined by tuple:  $(S,A,R,P,\gamma)$
    - $S$: set of possible states
    - $A$: set of possible actions
    - $R$: distribution of reward given (state, action) pair
    - $P$: transition probability i.e. distribution over next state given (state, action) pair
    - $\gamma$: discount factor
- Markov Decision Process
    - At time step t=0, environment samples initial state s0 ~ p(s0)
    - Then, for t=0 until done:
        - Agent selects action at
        - Environment samples reward $r_t ~ R(\cdot| s_t, a_t)$
        - Environment samples next state $s_{t+1}~ P( \cdot| s_t, a_t)$
        - Agent receives reward $r_t$ tand next state $s_{t+1}$
    - A policy $\pi$ is a function from S to A, that specifies what action to take in each state
    - Objective: find policy $\pi^*$ that maximizes cumulative discounted reward: $\sum\limits_{t \geq 0} \gamma^t r_t$
- Example
    
     Grid World
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20180.png)
    
- The optimal policy $\pi^*$
    
    We want to find optimal policy $\pi^*$ that maximizes the sum of rewards.
    How do we handle the randomness (initial state, transition probability…)?
    Maximize the expected sum of rewards. Formally:
    
    $\pi^{*} = \arg\max\limits_{\pi} \mathbb{E}\left[\sum\limits_{t \geq 0} \gamma^tr_t | \pi \right] \quad \text{with} \quad s_0 \sim p(s_0), a_t \sim \pi(\cdot|s_t), s_{t+1} \sim p(\cdot|s_t, a_t)$
    
- Value function and Q-value function
    
    Following a policy produces sample trajectories (or paths)  s0 , a0 , r0 , s1 , a1 , r1 , …
    
    How good is a state?
    The value function at state $s$, is the expected cumulative reward from following the policy
    from state s:
    
    $V^\pi(s) = \mathbb{E}\left[\sum\limits_{t \geq 0} \gamma^tr_t | s_0 = s, \pi\right]$
    
    How good is a state-action pair?
    The Q-value function at state $s$ and action $a$, is the expected cumulative reward from
    taking action $a$ in state $s$ and then following the policy:
    
    $Q^\pi(s,a) = \mathbb{E}\left[\sum\limits_{t \geq 0} \gamma^tr_t | s_0 = s, a_0 = a, \pi\right]$
    
    - Bellman equation
    The optimal Q-value function $Q^*$ is the maximum expected cumulative reward achievable
    from a given (state, action) pair:
        
        $Q^*(s,a) = \max\limits_\pi\mathbb{E}\left[\sum\limits_{t \geq 0} \gamma^tr_t | s_0 = s, a_0 = a, \pi\right]$
        
        $Q^*$ satisfies the following Bellman equation:
        
        $Q^*(s, a) = \mathbb{E}_{s' \sim \varepsilon} \left[ r + \gamma \max\limits_{a'} Q^*(s', a') \middle| s, a \right]$
        

Q-Learning

Policy Gradients

### Question

![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20181.png)

[lp spaces - What is rotation when we have a different distance metric? - Mathematics Stack Exchange](https://math.stackexchange.com/questions/2057140/what-is-rotation-when-we-have-a-different-distance-metric?answertab=votes#tab-top)

![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20182.png)

![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20183.png)

[7.6. 残差网络（ResNet） — 动手学深度学习 2.0.0 documentation (d2l.ai)](https://zh.d2l.ai/chapter_convolutional-modern/resnet.html#id1)
# CS231n

ËØæÁ®ãÔºö[CS231n](https://www.bilibili.com/video/BV1nJ411z7fe/)

- Relative resources
    
    [https://github.com/Na-moe/CS231n-2024](https://github.com/Na-moe/CS231n-2024)
    
    PPTÔºö[Index of /slides/2017 (stanford.edu)](https://cs231n.stanford.edu/slides/2017/)
    
    ÊúÄÊñ∞Ôºö[Stanford University CS231n: Deep Learning for Computer Vision](https://cs231n.stanford.edu/schedule.html)
    

## Lecture 1 Intro

Computer vision is the study of visual data.

CS231n focuses on one of the most important problems of visual recognition - **image classification (ÂõæÂÉèÂàÜÁ±ª)**

There is a number of visual recognition problems that are related to image classification, such as object detection (Áâ©‰ΩìÊ£ÄÊµã), image captioning (ÂõæÂÉèÊèèËø∞).

Convolutional Neural Networks (CNN) have become an important tool for object recognition

- Our philosophy
    - Thorough and Detailed.
        - Understand how to write from scratch, debug and train convolutional neural networks.
    - Practical.
        - Focus on practical techniques for training these networks at scale, and on GPUs(e.g. will touch on distributed optimization, differences between CPU vs. GPU, etc.)
        - Also look at state of the art software tools such as Caffe. TensorFlow,and (Py)Torch
    - State of the art.
        - Most materials are new from research world in the past 1-3 years. Very exciting stuff!
    - Fun.
        - Some fun topics such as lmage Captioning (using RNN)
        - Also DeepDream, NeuralStyle, etc.
- Pre-requisite
    - Proficiency in Python, some high-level familiarity with C/C++
        - All class assignments will be in Python (and use numpy), but some of the deep learning libraries we may look at later in the class are written in C++.
        - A Python tutorial available on course website
    - College Calculus, Linear Algebra
    - Equivalent knowledge of CS229 (Machine Learning)
        - We will be formulating cost functions, taking derivatives and performing ptimization with gradient descent.

## Lecture 2 Image Classification pipeline

Image Classification: A core task in Computer Vision

- Challenges
    - Viewpoint variation
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled.png)
        
    - Illumination
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%201.png)
        
    - Occlusion
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%202.png)
        
    - Background Clutter
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%203.png)
        

### An image classifier

- Hard-code rules
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%204.png)
    
- Data-Driven Approach
    - Steps
        - Collect a dataset of images and labels
        - use Machine Learning to train a dassifier
        - Evaluate the classifier on new images
    - Nearest Neighbor Classification
        
        (ÊúÄ‰∏¥ËøëÁÆóÊ≥ï)
        
        hyperparameters (choices about the algorithm that we set rather than learn): $K$ and the distance metric
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%205.png)
        
        - Distance Metric to compare images
            
            The L1 distance depends on your choice of coordinates system. So if you rotate the coordinate frame, that will change the L1 distance between points (doing this won‚Äôt influence the L2 distance)
            
            If there some individual entries of the input features vector have important meanings, L1 might be a more natural fit.
            
            However, if the features are just a generic vector in space, use L2 distance
            
            - L1 (Manhattan) distance $L1(\mathbf{x}, \mathbf{y}) = \sum\limits_{i} |x_i - y_i|$
                
                ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%206.png)
                
                ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%207.png)
                
            - L2 (Euclidean Ê¨ßÊ∞èË∑ùÁ¶ª) distance $L2(\mathbf{x}, \mathbf{y}) = \sqrt{\sum\limits_{i=1}^n (x_i - y_i)^2}$
            ÂØπÊØè‰∏™Áª¥Â∫¶‰∏äÁöÑÂùêÊ†áÂÄºÂÅöÂ∑ÆÔºåÁÑ∂ÂêéÂ∞ÜËøô‰∫õÂ∑ÆÂÄºÁöÑÂπ≥ÊñπÊ±ÇÂíåÔºåÊúÄÂêéÂØπËøô‰∏™ÂíåÂºÄÂπ≥ÊñπÊ†π„ÄÇËøô‰∏™ËøáÁ®ãÂÆûË¥®‰∏äËÆ°ÁÆóÁöÑÊòØ‰∏§ÁÇπÈó¥Áõ¥Á∫øË∑ùÁ¶ªÁöÑÊ¶ÇÂøµÔºåÂú®Êú∫Âô®Â≠¶‰π†ÂíåÁªüËÆ°Â≠¶‰∏≠Â∏∏Áî®‰∫éÂ∫¶ÈáèÊï∞ÊçÆÁÇπ‰πãÈó¥ÁöÑÁõ∏‰ººÊÄß„ÄÇ
            ÂÖ¨ÂºèÁ®ç‰ΩúÂèòÂΩ¢Ôºö ÂØπ‰∫éÊµãËØïÁÇπÈõÜ‰∏≠ÁöÑÊØè‰∏Ä‰∏™ÁÇπ $x_i$ ÂíåËÆ≠ÁªÉÁÇπÈõÜ‰∏≠ÁöÑÊØè‰∏Ä‰∏™ÁÇπ $x'_j$
            $d(\mathbf{x}_i, \mathbf{x}'_j) = \sqrt{\sum\limits_{k=1}^{n}(x_{ik}^2 + x_{jk}'^2 - 2x_{ik}x'_{jk})}=\sqrt{\sum\limits_{k=1}^{n}x_{ik}^2 + \sum\limits_{k=1}^{n}x_{jk}'^2 -\sum\limits_{k=1}^{n} 2x_{ik}x'_{jk}}$
                - $x_{ik}$¬†ÊòØÊµãËØïÁÇπ¬†$i$¬†Âú®Á¨¨¬†$k$¬†‰∏™Áª¥Â∫¶ÁöÑÂùêÊ†á
                - $x_{jk}'$ ÊòØËÆ≠ÁªÉÁÇπ¬†$j$¬†Âú®Á¨¨¬†$k$¬†‰∏™Áª¥Â∫¶ÁöÑÂùêÊ†á
                - $n$¬†ÊòØÁ©∫Èó¥ÁöÑÁª¥Â∫¶
                
                ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%208.png)
                
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%209.png)
            
        - Nearest Neighbor Classifier
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2010.png)
            
            Ê≥®ÔºöËÆ°ÁÆó distances Ëøô‰∏ÄË°å‰ΩøÁî®‰∫Ü numpy ÁöÑÂπøÊí≠Êú∫Âà∂
            
            Q: With N examples, how fast are training and prediction?
            A: Train $O(1)$, predict $O(N)$
            This is bad: we want classifiers that are fast at prediction; slow for training is ok
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2011.png)
            
            - The decision regions of a nearest neighbor classifier
            the points are training set, different colors represent different categories (class lable)
                
                ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2012.png)
                
        - K-Nearest Neighbors (KNN, K-ËøëÈÇªÁÆóÊ≥ï)
            
            Instead of copying label from the nearest neighbor, take **maiority vote**  (Â§öÊï∞ÊäïÁ•®) from $K$ closest neighbor points
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2013.png)
            
        - Setting Hyperparameters
            
            Keep a very strict separation between the val data and the test data is important.
            
            Choose hyperparameters using the **validation set**; only run on the test set once at the very end.
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2014.png)
            
            Our algorithm is able to see the labels of the training set, but for val set, algorithm can‚Äôt see the labels, we only use the labels of the val set to check the performance of our algorithm.
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2015.png)
            
        - Disadvantages
            - Very slow at test time
            - **Distance metrics on pixels are not informative**
                
                ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2016.png)
                
            - Curse of dimensionality
            We hope our training examples to cover the space densely (Otherwise the nearest neighbors could be quite far away from the testing points) 
            ‚Üí we need a number of training examples, which is exponential (ÊåáÊï∞) in the dimension of the problem.
                
                ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2017.png)
                
                (different colors in the picture mean different categories of the training set)
                
    - Linear Classification
        
        (Á∫øÊÄßÂàÜÁ±ª)
        
        - Parametric Approach
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2018.png)
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2019.png)
            
            - Linear Classification is a **template matching approach**.
                
                The **linear classifier only learns one template for each class**. 
                
                Each of the rows in the matrix $W$ correspond to the template of a class.
                
                (Neural networks and other more complex models don‚Äôt have the restriction of just learning a single template per category.)
                
        - Interpreting a Linear Classifier
            - Take the rows of weight matrix $W$ and unravel it back into an image to visualize:
                
                ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2020.png)
                
            - Linear Classification is learning linear decision boundaries between pixels in a high dimensional space
                
                ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2021.png)
                
        - Hard cases for a linear classifier
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2022.png)
            

## Lecture 3 Loss function and Optimization

### Loss function

- **Multiclass SVM loss**
    
    Multiclass SVM lossÔºöÂ§öÂàÜÁ±ª SVM ÊçüÂ§±ÂáΩÊï∞
    
    Given an example $(x_i, y_i)$, where $x_i$ is the image and $y_i$ is the real (integer) label (1~10)
    
    using the shorthand for the scores vector: $s = f(x_i, W)$
    
    the SVM loss (also called Hinge loss): 
    
    $$
    \begin{align*}
    L_i &= \sum_{j \neq y_i} \begin{cases} 0
     & \text{if } s_{y_i} \geq s_j + \Delta \\
      s_j - s_{y_i} + \Delta   & \text{otherwise}
    \end{cases} \\
    &= \sum\limits_{j \neq y_i} \max(0, s_j - s_{y_i} + \Delta)
    \end{align*}
    
    $$
    
    $s_j$ ÊòØÂàÜÁ±ªÂô®È¢ÑÊµãÂá∫Êù•ÁöÑÁ¨¨¬†$j$¬†‰∏™Á±ªÂà´ÁöÑÂàÜÊï∞Ôºå $s_{y_i}$¬†ÊòØÈ¢ÑÊµãÂá∫Êù•ÁöÑÊ≠£Á°ÆÁ±ªÂà´ÔºàÂç≥Ê†áÁ≠æÊâÄÂ±ûÁ±ªÂà´ÔºâÁöÑÂàÜÊï∞Ôºå $\Delta$ ÊòØÈòàÂÄºÔºàsafety marginÔºâÈÄöÂ∏∏‰∏∫ 1
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2023.png)
    
    - Python implement
        
        ```python
        import numpy as np
        
        def L_i_vectorized(x: np.ndarray, y: int, W: np.ndarray) -> float:
            """
            ËÆ°ÁÆóÂçï‰∏™Ê†∑Êú¨iÁöÑÊçüÂ§±ÂáΩÊï∞ÂÄºÔºåÈááÁî®ÂêëÈáèÂåñÊñπÊ≥ïÂÆûÁé∞„ÄÇ
            
            ÂèÇÊï∞:
            - x: ÁâπÂæÅÂêëÈáèÔºåÁ±ªÂûã‰∏∫numpy.ndarrayÔºåË°®Á§∫ËæìÂÖ•Êï∞ÊçÆÊ†∑Êú¨„ÄÇ
            - y: Ê†áÁ≠æÁ¥¢ÂºïÔºåÁ±ªÂûã‰∏∫intÔºåË°®Á§∫Ê†∑Êú¨ÁöÑÁúüÂÆûÊ†áÁ≠æÂú®ÂàÜÁ±ª‰∏≠ÁöÑÁ¥¢Âºï„ÄÇ
            - W: ÊùÉÈáçÁü©ÈòµÔºåÁ±ªÂûã‰∏∫numpy.ndarrayÔºåË°®Á§∫Ê®°ÂûãÁöÑÊùÉÈáçÂèÇÊï∞„ÄÇ
            
            ËøîÂõû:
            - loss_i: Âçï‰∏™Ê†∑Êú¨iÁöÑÊçüÂ§±ÂÄºÔºåÁ±ªÂûã‰∏∫float„ÄÇ
            """
            # ËÆ°ÁÆóÁ∫øÊÄßÂàÜÊï∞
            scores = W.dot(x)
            
            # ËÆ°ÁÆóËæπÈôÖÊçüÂ§±ÔºåÊ≥®ÊÑèÊ≠£Á°ÆÂàÜÁ±ªÁöÑËæπÈôÖÊçüÂ§±Âº∫Âà∂ËÆæ‰∏∫0
            margins = np.maximum(0, scores - scores[y] + 1)
            margins[y] = 0
            
            # ËÆ°ÁÆóÊçüÂ§±
            loss_i = np.sum(margins)
            
            return loss_i
        ```
        
    - Q&A:
        - the min of Multiclass SVM loss is **0** (È¢ÑÊµãÂá∫ÁöÑÊ≠£Á°ÆÁ±ªÂà´ÁöÑÂàÜÊï∞ÊØîÈ¢ÑÊµãÂá∫ÁöÑÂÖ∂‰ªñÁ±ªÂà´ÁöÑÂàÜÊï∞ÈÉΩÈ´òÂá∫ÈòàÂÄº) and the max is **infinity**
        - Q: At initialization $W$ is small so all $s \approx 0$, What is the loss?
        A: $L =(c-1)\Delta$, where $c$ is the number of classes
        - Q: What if the sum when calculating $L_i$ was over all classes (including $j=y_i$)?
        A: The $L_i$ increases by $\Delta$ (Ê≠£Á°ÆÁ±ªÂà´ÁöÑ $L_i=0+\Delta=\Delta$)
        - Q: What if we used mean instead of sum when calculating $L_i$?
        A: Doesn‚Äôt change, beacuse we actually don‚Äôt care about the true values of the loss.
        - Q: What if we used $L_i= \sum\limits_{j \neq y_i} \max(0, s_j - s_{y_i} + \Delta)^2$
        A: Different. The point of a loss function is to quantify how bad are different categories of mistakes,
        - Q: Suppose that we found a $W$ such that $L=0$, is this $W$ unique?
        A: No, $2W$ is also lead to $L=0$ (the margins between the correct and incorrect scores will also double, since they were already greater than $\Delta$, they still greater than $\Delta$ now)
    - gradient $dW$
        
        ÂØπ‰∫é $s = f(x_i; W) = Wx_i$
        
        ÂØπ‰∫é $s_j$ ÁöÑËÆ°ÁÆóÔºåÂè™ÊúâÂΩì $x_{ik}$ÔºàÂç≥ $x_i$ ÁöÑÁ¨¨ $k$ ‰∏™ÂÖÉÁ¥†Ôºâ‰∏éÊùÉÈáçÁü©Èòµ $W$ ÁöÑÂØπÂ∫îÂÖÉÁ¥†Áõ∏‰πòÊó∂Êâç‰ºöË¥°ÁåÆÔºåÂç≥ $s_j = \sum\limits_{k=1}^{D} W_{jk}x_{ik}$ÔºåÂõ†Ê≠§ $\frac{\partial s_j}{\partial W_{jk}} = x_{ik}$„ÄÇÂØπ‰∫éÂÖ∂‰ªñÁöÑ $W_{lm}$Ôºà $l \neq j$ Êàñ $m \neq k$ÔºâÔºåËøô‰∏™ÂÅèÂØºÊï∞‰∏∫Èõ∂ÔºåÂõ†‰∏∫ÂÆÉ‰ª¨‰∏çÂΩ±Âìç $s_j$
        
- Regularization term
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2024.png)
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2025.png)
    
    Regularization hyper-parameter $\lambda$ trades off between the two
    
    - Intuition
        
        When doing a regression problem in terms of different polynomial basis functions, maybe the model has access to polynomials of very high degree, by adding this regression penalty, can encourage the model to prefer polynomials of lower degree, if they fit the data properly.
        
        If the model want to use these more complex item (like high degrees of polynomial), it need to overcome the penalty.
        
    - Types
        - L2 regularization $R(W) = \sum\limits_k \sum\limits_l W_{k,l}^2$
        ÂÆûÁé∞Ôºönp.sum(W * W)
        - L1 regularization $R(W) = \sum\limits_k \sum\limits_l |W_{k,l}|$
        - Elastic net (L1 + L2) $R(W) = \sum\limits_k \sum\limits_l \beta W_{k,l}^2 + |W_{k,l}|$
        - Max norm regularization (might see later)
        - Dropout (will see later)
        - Fancier: Batch normalization, stochastic depth
    - Difference between L2 and L1 regularization
        - L2 regularization prefers to spread the influence across all the different values in feature $x$, the decisions tend to spread out and depend on the entire $x$ vector.
        - L1 regularization prefers sparse (Á®ÄÁñè) solutions that can drives most of the entries of $W$ to 0 and allows a few to deviate from 0.
- Softmax Classifier
    
    also called **Multinomial Logistic Regression**
    
    - scores: $s=f(x_i;W)$
    scores we get by linear classification is **unnormalized log probabilities** of the classes, $s$ ÊòØ‰∏Ä‰∏™Ê†áÈáèÂÄºÔºåË°®Á§∫ÁΩëÁªúÔºàÊàñÊ®°ÂûãÔºâÂú®ÊùÉÈáç $W$ ÂØπËæìÂÖ• $x_i$ Â§ÑÁêÜÂêéÂæóÂà∞ÁöÑÂØπ‰∫éÁ±ªÂà´ $k$ ÁöÑÊú™ÂΩí‰∏ÄÂåñÂæóÂàÜÔºàlogitÔºâ
    - $P(Y=k|X=x_i) = \frac{e^{s_k}}{\sum\limits_{j=1}^C e^{s_j}}$ 
    Ë°®Á§∫Âú®ÁªôÂÆöËæìÂÖ•ÁâπÂæÅÂêëÈáè $x_i$ ÁöÑÊù°‰ª∂‰∏ãÔºåÈ¢ÑÊµãÁ±ªÂà´ $Y$ ‰∏∫Á¨¨ $k$ Á±ªÁöÑÊ¶ÇÁéáÔºå
    - $L_i=-\log P(Y=y_i|X=x_i)$
    ÁõÆÊ†áÊòØÊúÄÂ§ßÂåñÊï¥‰∏™Êï∞ÊçÆÈõÜÁöÑ‰ººÁÑ∂ÔºàlikelihoodÔºâÔºåÊàñËÄÖÁ≠âÊïàÂú∞ÔºåÊúÄÂ∞èÂåñË¥üÂØπÊï∞‰ººÁÑ∂ÊçüÂ§±
    - In summary: $L_i=-\log\left(\frac{e^{s_{y_i}}}{\sum_{j=1}^C e^{s_j}}\right)= -s_{y_i} + \log\left(\sum\limits_{j=1}^C e^{s_j}\right)$
    
    Ëøô‰∏™Ë°®ËææÂºèÈºìÂä±Ê®°ÂûãÊèêÈ´òÊ≠£Á°ÆÂàÜÁ±ªÁöÑÂæóÂàÜÔºåÂêåÊó∂Áõ∏ÂØπÈôç‰ΩéÂÖ∂‰ªñÂàÜÁ±ªÁöÑÂæóÂàÜÔºå‰ªéËÄåËææÂà∞ÂàÜÁ±ªÁöÑÁõÆÁöÑ
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2026.png)
    
    - Q&A:
        - theoretically, the min of loss is **0** and the max is **plus infinity**
        - Q: At initialization $W$ is small so all $s \approx 0$, What is the loss?
        A: $L=-log(\frac{1}{c})=log(c)$, where $c$ is the number of classes
    - gradient
        - $L-s$
            
            $L_i$ÂØπ‰∫é $s_{y_i}$ ÁöÑÂÅèÂØºÊï∞ $\frac{\partial L_i}{\partial s_{y_i}} = -1 + \frac{e^{s_{y_i}}}{\sum\limits_{j=1}^C e^{s_j}} =  -1 + P(Y=y_i|X=x_i)$
            
            - ÂÖ∑‰ΩìÊ±ÇÊ≥ï
                
                ÂΩìÁÑ∂ÔºåËÆ©Êàë‰ª¨‰∏ÄÊ≠•Ê≠•Êù•Ëß£ÊûêËøô‰∏™ÂØºÊï∞ÁöÑÊé®ÂØºËøáÁ®ã„ÄÇÂ∑≤Áü•ÊçüÂ§±ÂáΩÊï∞ $L_i$ Â¶Ç‰∏ãÔºö$L_i = -\log\left(\frac{e^{s_{y_i}}}{\sum_{j=1}^C e^{s_j}}\right)$ÔºåËÆ∞ $u = \frac{e^{s_{y_i}}}{\sum_{j=1}^C e^{s_j}}$
                
                ÈÇ£‰πàÊúâÔºö $\frac{\partial L_i}{\partial s_{y_i}} = \frac{\partial}{\partial s_{y_i}}(-\log(u)) = -\frac{1}{u} \cdot \frac{\partial u}{\partial s_{y_i}}$
                
                Êé•‰∏ãÊù•ÔºåÊàë‰ª¨ÈúÄË¶ÅËÆ°ÁÆó $u$ ÂÖ≥‰∫é $s_{y_i}$ ÁöÑÂØºÊï∞ÔºåÊ≥®ÊÑèÂà∞ $\frac{\partial}{\partial s_{y_i}}(s_{y_i}) = 1$ ËÄå $\frac{\partial}{\partial s_{y_i}}(e^{s_j}) = 0$ ÂØπ‰∫éÊâÄÊúâ $j \neq y_i$Ôºå
                
                $\frac{\partial u}{\partial s_{y_i}} = \frac{(\sum\limits_{j=1}^C e^{s_j} )\cdot e^{s_{y_i}} \cdot \frac{\partial}{\partial s_{y_i}}(s_{y_i}) - e^{s_{y_i}} \cdot  \frac{\partial}{\partial s_{y_i}}(\sum\limits_{j=1}^C e^{s_j})}{(\sum\limits_{j=1}^C e^{s_j})^2}= \frac{e^{s_{y_i}} \cdot \sum\limits_{j=1}^C e^{s_j} - e^{s_{y_i}} \cdot e^{s_{y_i}}}{(\sum\limits_{j=1}^C e^{s_j})^2} = \frac{e^{s_{y_i}}}={\sum\limits_{j=1}^C e^{s_j}} - \frac{e^{2s_{y_i}}}{(\sum\limits_{j=1}^C e^{s_j})^2}=\frac{e^{s_{y_i}}}{\sum\limits_{j=1}^C e^{s_j}} \cdot \left(1 - \frac{e^{s_{y_i}}}{\sum\limits_{j=1}^C e^{s_j}}\right)$
                
                ËøôÂÆûÈôÖ‰∏äÂ∞±ÊòØÔºö$\frac{\partial u}{\partial s_{y_i}} = u \cdot (1 - u)$
                
                ÂõûÂà∞ÂéüÂßãÁöÑÂØºÊï∞ËÆ°ÁÆó‰∏≠Ôºå‰ª£ÂÖ• $u$ Âíå $\frac{\partial u}{\partial s_{y_i}}$Ôºö$\frac{\partial L_i}{\partial s_{y_i}} = -\frac{1}{u} \cdot u \cdot (1 - u)  = -1 + u$
                
            
            $j \neq y_i$ Êó∂Ôºå $\frac{\partial L_i}{\partial s_j} = \frac{1}{\sum\limits_{m=1}^C e^{s_m}} \cdot e^{s_j} = P(Y=j|X=x_i)$
            
        - $L - W$
            
            ÂØπ‰∫é $s = f(x_i; W) = Wx_i$
            
            ÂØπ‰∫é $s_j$ ÁöÑËÆ°ÁÆóÔºåÂè™ÊúâÂΩì $x_{ik}$ÔºàÂç≥ $x_i$ ÁöÑÁ¨¨ $k$ ‰∏™ÂÖÉÁ¥†Ôºâ‰∏éÊùÉÈáçÁü©Èòµ $W$ ÁöÑÂØπÂ∫îÂÖÉÁ¥†Áõ∏‰πòÊó∂Êâç‰ºöË¥°ÁåÆÔºåÂç≥ $s_j = \sum\limits_{k=1}^{D} W_{jk}x_{ik}$ÔºåÂõ†Ê≠§ $\frac{\partial s_j}{\partial W_{jk}} = x_{ik}$„ÄÇÂØπ‰∫éÂÖ∂‰ªñÁöÑ $W_{lm}$Ôºà $l \neq j$ Êàñ $m \neq k$ÔºâÔºåËøô‰∏™ÂÅèÂØºÊï∞‰∏∫Èõ∂ÔºåÂõ†‰∏∫ÂÆÉ‰ª¨‰∏çÂΩ±Âìç $s_j$
            
            Â∫îÁî®ÈìæÂºèÊ≥ïÂàô $\frac{\partial L_i}{\partial W_{jk}}=\frac{\partial L_i}{\partial s_k} \cdot \frac{\partial s_j}{\partial W_{jk}}$
            
            ÂØπ‰∫é $j =y_i$Ôºå  $\frac{\partial L_i}{\partial w_{jk}} = x_{ik} \cdot (p_{y_i}-1 )$
            
            ÂØπ‰∫é $j \neq y_i$Ôºå $\frac{\partial L_i}{\partial w_{kj}} = x_{ik} \cdot p_j$
            
            Ê±áÊÄªÔºåÂæóÂà∞Êï¥‰∏™Êï∞ÊçÆÈõÜ‰∏äÁöÑÂπ≥ÂùáÊ¢ØÂ∫¶ÔºàÈÄêÂÖÉÁ¥†ÂΩ¢ÂºèÔºâ
            
            $\frac{\partial L}{\partial w_{jk}} = \frac{1}{N} \sum\limits_{i=1}^{N} \left( x_{ik} (p_j - \textbf{1}_{j=y_i}) \right)$
            
            Âç≥ $\frac{\partial L}{\partial W} = \frac{1}{N} \cdot X^T \cdot (P - Y)$
            
            - Êé®ÂØº
                - Á¨¶Âè∑ÊÑè‰πâ:
                    - $X$ ÊòØ‰∏Ä‰∏™ $N \times D$ ÁöÑÁü©ÈòµÔºåÂÖ∂‰∏≠ÊØè‰∏ÄË°å‰ª£Ë°®‰∏Ä‰∏™Ê†∑Êú¨ÁöÑÁâπÂæÅÂêëÈáè $x_i$
                    - $P$ ÊòØ‰∏Ä‰∏™ $N \times C$ ÁöÑÁü©ÈòµÔºåÊØè‰∏ÄË°åÂåÖÂê´Ê†∑Êú¨Âú®ÊâÄÊúâÁ±ªÂà´‰∏äÁöÑÈ¢ÑÊµãÊ¶ÇÁéáÂêëÈáè $(p_1, p_2, ..., p_C)$
                    - $Y$ ÊòØ‰∏Ä‰∏™ $N \times C$ ÁöÑÁã¨ÁÉ≠ÁºñÁ†ÅÁü©ÈòµÔºåÂ¶ÇÊûúÁ¨¨ $i$ ‰∏™Ê†∑Êú¨Â±û‰∫éÁ¨¨ $c$ Á±ªÔºåÂàô $Y_{ic}=1$ÔºåÂÖ∂‰ªñ‰ΩçÁΩÆ‰∏∫ $0$
                    - $w_{jk}$ ÊòØÊùÉÈáçÁü©Èòµ $W$ ‰∏≠ÁöÑ‰∏Ä‰∏™ÂÖÉÁ¥†ÔºåÂØπÂ∫î‰∫é‰ªéÁ¨¨ $k$ ‰∏™ÁâπÂæÅÂà∞Á¨¨ $j$ ‰∏™Á±ªÂà´ÁöÑÊùÉÈáç
                - ÈÄêÂÖÉÁ¥†Êé®ÂØºÂà∞Áü©ÈòµËøêÁÆó
                    - Âú®ÈÄêÂÖÉÁ¥†ÂÖ¨Âºè‰∏≠Ôºå $\frac{\partial L}{\partial w_{jk}}$ Ë°®Á§∫ÊùÉÈáç $W$ ÁöÑÁ¨¨ $jk$ ‰∏™ÂÖÉÁ¥†ÂÖ≥‰∫éÊçüÂ§±ÂáΩÊï∞ $L$ ÁöÑÂÅèÂØºÊï∞ÔºåÂÆÉÊòØÈÄöËøáÂØπÊâÄÊúâÊ†∑Êú¨ $i$ ÁöÑÊüê‰∏™ÁâπÂÆöÊìç‰ΩúÊ±ÇÂíåÂæóÂà∞ÁöÑ„ÄÇ
                    - ÂØπ‰∫éÊâÄÊúâÁöÑ $j$ Âíå $k$ÔºåËØ•Êìç‰ΩúÊ∂âÂèäÂà∞‰πò‰ª• $x_{ik}$ÔºàÁ¨¨ $i$ ‰∏™Ê†∑Êú¨ÁöÑÁ¨¨ $k$ ‰∏™ÁâπÂæÅÂÄºÔºâÔºåÁÑ∂Âêé‰πò‰ª• $(p_j - \textbf{1}_{j=y_i})$ÔºåÂêéËÄÖÊòØÁ¨¨ $j$ Á±ªÁöÑÊ¶ÇÁéáÂáèÂéª‰∏Ä‰∏™ÊåáÁ§∫ÂáΩÊï∞ÔºàËã• $j$ ÊòØÊ†∑Êú¨ $i$ ÁöÑÁúüÂÆûÁ±ªÂà´Âàô‰∏∫1ÔºåÂê¶Âàô‰∏∫0Ôºâ„ÄÇ
                - ËΩ¨Êç¢Âà∞Áü©Èòµ‰πòÊ≥ï:
                    - ÂÖ≥ÈîÆÂú®‰∫éÁêÜËß£ $P - Y$ Ëøô‰∏ÄÊ≠•„ÄÇ$$P$$ ÊòØÊâÄÊúâÊ†∑Êú¨Âú®ÊâÄÊúâÁ±ªÂà´‰∏äÁöÑÈ¢ÑÊµãÊ¶ÇÁéáÁü©ÈòµÔºåËÄå $Y$ ÊòØÁã¨ÁÉ≠ÁºñÁ†ÅÁöÑÊ†áÁ≠æÁü©Èòµ„ÄÇ $P - Y$ ÂÆûÈôÖ‰∏äÊòØ‰∏∫ÊØè‰∏ÄË°åÔºàÊØè‰∏™Ê†∑Êú¨ÔºâÂàõÂª∫‰∫Ü‰∏Ä‰∏™ÂêëÈáèÔºåÂÖ∂‰∏≠ÊØè‰∏™ÂÖÉÁ¥†Ë°®Á§∫ËØ•Á±ªÂà´ÁöÑÈ¢ÑÊµãÊ¶ÇÁéáÂáèÂéªËØ•Ê†∑Êú¨ÊòØÂê¶Â±û‰∫éËØ•Á±ªÂà´ÁöÑÊ†áËÆ∞ÔºàÂç≥ $p_j - \textbf{1}_{j=y_i}$Ôºâ„ÄÇ
                    - Áî±‰∫éÊàë‰ª¨ÈúÄË¶ÅÂØπÊâÄÊúâÊ†∑Êú¨ÁöÑÁõ∏ÂêåÁâπÂæÅ $k$ ÂÅöËøôÈ°πÊìç‰ΩúÔºåÊàë‰ª¨ÂèØ‰ª•Â∞Ü $X$ ÁöÑËΩ¨ÁΩÆ $X^T$ ‰∏é $(P - Y)$ Áõ∏‰πò„ÄÇ$$X^T$$ ÁöÑÊØè‰∏ÄÂàóÂ∞±ÊòØÁâπÂæÅ $k$ Âú®ÊâÄÊúâÊ†∑Êú¨‰∏äÁöÑÂÄºÔºå‰∏é $(P - Y)$ Áõ∏‰πòÔºåÁõ∏ÂΩì‰∫éÂØπÊâÄÊúâÊ†∑Êú¨ÊâßË°å‰∏äËø∞ÈÄêÂÖÉÁ¥†Êìç‰ΩúÂπ∂Ê≤øÁâπÂæÅÁª¥Â∫¶Ê±ÇÂíå„ÄÇ
- Difference between the two loss functions
    - One aspect:
        
        The difference is how we choose to interpret those scores, to quantitatively measure the badness afterwards.
        
        - For SVM loss, we look at the margins between the scores of the correct class and those of the incorrect class.
        - For cross-entropy loss, we compute a probability distriution, and then look at the **minus log probability** of the correct class.
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2027.png)
        
    - Another aspect
        - The SVM loss only care about getting correct score to be greater than a margin above the incorrect scores ‚Üí get one data point over the bar and then doesn‚Äôt care about this data point any more.
        - The softmax loss always drive the score of the correct class towards infinity and the score of the incorrect classes down towards minus infinity ‚Üí always try to **continually improve every single data point** to get better.
- Recap
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2028.png)
    

### Optimization

- Follow the slope
    
    In 1-dimension, the derivative of a function: $\frac{df(x)}{dx} = \lim\limits_{{h \to 0}} \frac{f(x+h) - f(x)}{h}$
    
    In multiple dimensions, the gradient is the vector of partial derivatives along each dimension
    The slope in any direction is the dot product of the unit vector of that direction with the gradient
    The direction of steepest descent is the nenative gradient
    
- Use calculus to compute an analytic gradient
    - Numerical gradient (ÊúâÈôêÂ∑ÆÂàÜ‰º∞ËÆ°): approximate, slow, easy to write
    - Analytic gradient (Ëß£ÊûêÊ¢ØÂ∫¶ËÆ°ÁÆó): exact, fast, error-prone
    - In practice: Always use analytic gradient, but check implementation with numerical gradient. This is called a gradient check.
- Stochastic Gradient Descent (SGD)
    
    At every iteration, we sample some small set of training examples, called a **mini-batch**, to compute an estimate of the full sum and the true gradient.
    
    Commonly 32/64/128 examples per mini-batch
    

## Lecture 4 Backpropagation and Neural Networks

### Computational graphs and **backpropagation**

How to compute the analytic gradient for arbitrarily complex functions?

Using a framework called **computational graphs** to express a function, then we can use **backpropagation**, which recursively use the **chain rule** to compute the gradient with respect to every variable in the computational graph.

![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2029.png)

What we need to deal with is just the **local gradient**, then use the chain rule to numerically multiply this all the way backwards and get the gradients of all the parameters.

- example
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2030.png)
    
    - Computational graph representation may not be unique
        
        Choose one where local gradients at each node can be easily expressed.
        
        Sigmoid local gradient: $\frac{d\sigma(x)}{dx} = \frac{e^{-x}}{(1+e^{-x})^2} = \left(\frac{1+e^{-x}-1}{1+e^{-x}}\right)\left(\frac{1}{1+e^{-x}}\right) = (1-\sigma(x))\sigma(x)$
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2031.png)
        
- Patterns in backward flow
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2032.png)
    
- Gradients for vector
    
    when $x,y,z$ are vectors, $\frac{\partial z}{\partial x},\frac{\partial z}{\partial y}$ are Jacobian matrix now
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2033.png)
    
    ÈõÖÂèØÊØîÁü©ÈòµÔºàJacobian matrixÔºâÊòØÂêëÈáèÂÄºÂáΩÊï∞ÁöÑ‰∏ÄÈò∂ÂÅèÂØºÊï∞ÁªÑÊàêÁöÑÁü©Èòµ„ÄÇÂØπ‰∫é‰∏Ä‰∏™‰ªé $\mathbb{R}^n$ Âà∞ $\mathbb{R}^m$ ÁöÑÂáΩÊï∞ $\mathbf{f} : \mathbb{R}^n \rightarrow \mathbb{R}^m$ÔºåÂÖ∂ÈõÖÂèØÊØîÁü©Èòµ $\mathbf{J}$ ÊòØ‰∏Ä‰∏™ $m \times n$ ÁöÑÁü©Èòµ„ÄÇÁü©Èòµ‰∏≠ÁöÑÁ¨¨ $i$ Ë°åÁ¨¨ $i$ ÂàóÂÖÉÁ¥†ÊòØÂáΩÊï∞ $\mathbf{f}$ ÁöÑÁ¨¨ $i$ ‰∏™ÂàÜÈáèÂáΩÊï∞ $f_i$ ÂØπÁ¨¨ $j$ ‰∏™ÂèòÈáè $x_j$ ÁöÑÂÅèÂØºÊï∞„ÄÇ
    
    - example:  ReLU ÊøÄÊ¥ªÂáΩÊï∞
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2034.png)
        
        $\frac{\partial L}{\partial x} = \frac{\partial L}{\partial f} \cdot \frac{\partial f}{\partial x}$ where $\mathbf{J}=\frac{\partial f}{\partial x}$ is a 4096 x 4096 Jacobian matrix
        
        ÂØπ‰∫éËøô‰∏™‰æãÂ≠êÔºö
        
        $\mathbf{J} = \begin{pmatrix}
        \frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} & \cdots & \frac{\partial f_1}{\partial x_{4096}} \\
        \frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2} & \cdots & \frac{\partial f_2}{\partial x_{4096}} \\
        \vdots & \vdots & \ddots & \vdots \\
        \frac{\partial f_{4096}}{\partial x_1} & \frac{\partial f_{4096}}{\partial x_2} & \cdots & \frac{\partial f_{4096}}{\partial x_{4096}}
        \end{pmatrix}$
        
        $\mathbf{J}$ is a diagonal matrix since each element of output $f_i(x)$ is affected by the corresponding element of input $x_i$.
        
        $\frac{\partial f_i}{\partial x_i} =
        \begin{cases}
        1, & \text{if } x_i > 0 \\
        0, & \text{if } x_i \leq 0
        \end{cases}$
        
    - example: $L_2$ function
        
        $f(x, W) = \|W \cdot x\|^2 = \sum\limits_{i=1}^n (W\cdot{x})_i^2$
        where $x \in \mathbb{R}^n,\ W \in \mathbb{R}^{n \times n}$
        
        ‰∏≠Èó¥ËøáÁ®ãÔºö
        
        $q = W \cdot x =
        \begin{pmatrix}
        W_{1,1} x_1 + \cdots + W_{1,n} x_n \\
        \vdots \\
        W_{n,1} x_1 + \cdots + W_{n,n} x_n
        \end{pmatrix}=
        \begin{pmatrix}
        q_1 \\
        \vdots \\
        q_n
        \end{pmatrix},\ f(q) = \|q\|^2 = q_1^2 + \cdots + q_n^2$
        
        Ê±ÇÂÅèÂØºÔºö 
        
        $\frac{\partial f}{\partial q_i} = \frac{\partial}{\partial q_i} (q_1^2 + q_2^2 + \cdots + q_i^2 + \cdots + q_n^2)= 2q_i$  ‚áí $\nabla_q f=2q$
        
        Âç≥Ôºö$L2$ ËåÉÊï∞Âπ≥ÊñπÂáΩÊï∞ÂÖ≥‰∫éÂÖ∂ËæìÂÖ•ÂêëÈáèÁöÑÊ¢ØÂ∫¶ÊòØËØ•ËæìÂÖ•ÂêëÈáèÁöÑ‰∏§ÂÄç
        
        $\frac{\partial q_k}{\partial W_{i,j}} = \textbf{1}_{k=i}x_j$
        
        $\textbf{1}_{k=i}$ ÊòØ‰∏Ä‰∏™ÊåáÁ§∫ÂáΩÊï∞ÔºåÂΩì $ùëò=ùëñ$ Êó∂ÂÄº‰∏∫ 1ÔºåÂê¶Âàô‰∏∫ 0
        
        $\frac{\partial f}{\partial W_{i,j}} = \sum\limits_k \left(\frac{\partial f}{\partial q_k}\cdot \frac{\partial q_k}{\partial W_{i,j}}\right) = \sum\limits_k (2q_k) (1_{k=i}x_j) = 2q_i x_j$ ‚áí $\nabla_wf = 2q \cdot x^T$
        
        ‚ÄúÊ±ÇÂíå‚ÄùÁ°Æ‰øù‰∫ÜÊâÄÊúâÂΩ±Âìç $f$ ÂÄºÁöÑË∑ØÂæÑÈÉΩË¢´ËÄÉËôëËøõÊù•
        
        $\frac{\partial q_k}{\partial x_i} = W_{k,i}$
        
        $\frac{\partial f}{\partial x_i} = \sum\limits_k \frac{\partial f}{\partial q_k} \frac{\partial q_k}{\partial x_i} = \sum\limits_k 2q_k W_{k,i}$ ‚áí $\nabla_x f = 2W^T \cdot q$
        
        Always check: The gradient with respect to a variable should have the same shape as the variable
        
- Summary
    - neural nets will be very large: impractical to write down gradient formula by hand for all parameters
    - in order to get these gradients, backpropagation = recursive application of the chain rule along a computational graph to compute the gradients of all inputs/ parameters/ intermediates
    - implementations maintain a graph structure, where the nodes implement the **forward()**/ **backward()** API
        - forward: compute result of an operation and save any intermediates needed for gradient computation in memory
        - backward: apply the chain rule to compute the gradient of the loss function with respect to the inputs

### Neural Networks

- Functional perspective: without the brain stuff
    
    Before: Linear score function $f = Wx$
    
    Now: 2-layer Neural Network $f = W_2 \max(0, W_1 x)=W_2h$
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2035.png)
    
    $h$ is the value of scores of the templates in $W_1$, and $W_2$ to weight all of the intermediate scores to get final score for the classes
    
    or 3-layer Neural Network $f = W_3 \max(0, W_2 \max(0, W_1 x))$
    

![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2036.png)

- Be very careful with your brain analogies!
    
    Biological Neurons:
    
    - Many different types
    - Dendrites (Ê†ëÁ™Å) can perform complex non-linear computations
    - Synapses (Á™ÅËß¶) are not a single weight but a complex non-inear dynamical system (Âä®ÊÄÅÁöÑÈùûÁ∫øÊÄßÁ≥ªÁªü)
    - Rate code may not be adequate
- Neural networks: Architectures
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2037.png)
    
- Summary
    - We arrange neurons into fully-connected layers
    - The abstraction of a **layer** has the nice property that it allows us to use efficient vectorized code (e.g. matrix multiplies)
    - Neural networks are not really neural
- Summary
    
    Process: Mini-batch Stochastic Gradient Descent
    
    Loop:
    
    - **Sample** a batch of data
    - **Forward** prop it through the computational graph (neural network), and get loss
    - **Backprop** to calculate the gradients
    - **Update** the parameters using the gradient

## Lecture 5 Convolutional Neural Networks

[[ConvNetJS demo: training on CIFAR-10]](http://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html)

### Fully Connected Layer

input: 32x32x3 image ‚Üí stretch to 3072x1

![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2038.png)

Each neuron looks at the full input volume.

Instead of preserving spatial structure, at the last layer at the end, we aggregate all of this together to draw some conclusion.

### Convolutional Layer

Difference: it can preserve spatial structure (Á©∫Èó¥ÁªìÊûÑ)

- Intro
    
    input: 32x32x3 image ‚Üí preserve spatial structure
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2039.png)
    
    Convolve the filter (Âç∑ÁßØÊ†∏) with the image, i.e. slide over the image spatially computing dot products
    
    Filters always extend the **depth** of the input volume
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2040.png)
    
    - definition of convolution of two signals
        
        elementwise multiplication and sum of a filter and the signal (image)
        
        $f[x,y]*g[x,y] = \sum\limits_{n_1=-\infty}^{\infty} \sum\limits_{n_2=-\infty}^{\infty} f[n_1,n_2] \cdot g[x-n_1,y-n_2]$
        
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2041.png)
    
    - The brain/neuron view of CONV Layer
        
        An activation map is a 28x28 sheet of neuron
        outputs:
        
        - Each is connected to a small region in the input (rather than looking at the whole input by the Fully Connected Layer)
        - All of them share parameters
        
        a 5x5 filter ‚áí 5x5 receptive field for each neuron
        
    
    consider a second, green filter to learn another specific type of template or concept in the input volumn
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2042.png)
    
    For example, if we had 6 5x5x3 filters, we'll get 6 separate activation maps
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/4843a797-d17d-4fdf-9b1a-c7955ec0f539.png)
    
    We stack these up to get a‚Äúnew image" of size 28x28x6
    
    - The brain/neuron view of CONV Layer
        
        CONV layer consists of neurons arranged in a 3D grid (28x28x6)
        There will be 6 different neurons all looking at the same region in the input volume looking for different things.
        
    
    Preview: ConvNet is a sequence of Convolutional Layers, interspersed with activation functions
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2043.png)
    
    E.g. 32x32 input convolved repeatedly with 5x5 filters shrinks volumes spatially!
    (32 -> 28 -> 24 ...). Shrinking too fast is not good, doesn‚Äôt work well
    
- Aside: Image Processing via Convolutions
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2044.png)
    
- What do convolutional filters learn
    
    ![Each element of these grids is showing what in the input would basically maximizes the activation of the neuron, i.e. what is the neuron looking for.](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2045.png)
    
    Each element of these grids is showing what in the input would basically maximizes the activation of the neuron, i.e. what is the neuron looking for.
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2046.png)
    
- the spatial dimensions of activation map
    
    notation: $F$ = filter size, $N$ = length of input, $P$ = padding, $S$ = stride
    
    - Without padding
    Output size: $(N-F)/S+ 1$
    e.g. $N=7,F=3$:
        - stride 1 ‚áí (7-3)/1+1=5
        - stride 2 ‚áí (7-3)/2+1=3
        - stride 3 ‚áí (7-3)/3+ 1=2.33 don‚Äôt fit
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2047.png)
        
    - Common to zero pad the border
    Output size:  $(N+2P-F)/S+ 1$
        - e.g. input 7x7, use 3x3 filter, applied with stride 1, pad with 1 pixel border ‚áí 7x7 output
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2048.png)
        
    - in general, common to see CONV layers with stride 1, filters of size FxF, and zero-padding with $(F-1)/2$
    (will preserve the same size spatially from input to output)
        - F = 3 ‚áí zero pad with 1
        - F = 5 ‚áí zero pad with 2
        - F = 7 ‚áí zero pad with 3
    - example
        
        Input volume: 32x32x3, using 10 5x5 filters with stride 1, pad 2 ‚áí output: 32x32x10
        
        The number of parameters in this layer: 10x(5x5x3+1)=760
        
        (each filter has a bais term)
        
- 1x1 convolution layers also make sense
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2049.png)
    

### Pooling layer

- To makes the representations smaller and more manageable
- operates over each activation map independently
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2050.png)
    
- Commonly use: max pooling
    - No learnable parameters
    - Introduces spatial invariance
    - It‚Äôs commen to set up the stride to have them not have any overlap
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2051.png)
    
- spatial dimensions
    
    assume input is $W_1\times H_1\times  C$
    Pooling layer needs 2 hyperparameters:
    
    - The spatial extent $F$
    - The stride $S$
    
    This will produce an output of $W_2 \times H_2\times C$ where:
    
    - $W_2 = (W_1 - F )/S + 1$
    - $H_2 = (H_1 - F)/S + 1$
    
    Number of parameters: 0
    
    Common settings:
    $F=2,S=2$
    $F=3,S=2$
    
- Summary
    - ConvNets stack CONV,POOL,FC layers
    - Trend towards smaller filters and deeper architectures
    - Trend towards getting rid of POOL/FC layers (just CONV)
    - Historically architectures looked like
    $[(\text{CONV}-\text{RELU})^ N-\text{POOL}?]^M-(\text{FC}-\text{RELU})^K,\text{SOFTMAX}$
    where N is usually up to ~5, M is large, 0 ‚â§ K ‚â§ 2
    - But recent advances such as ResNet/GoogLeNet have challenged this paradigm

## Lecture 6 Training Neural Networks Part 1

### Activation Functions

![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2052.png)

![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2053.png)

- Sigmoid
    - Squashes numbers to range (0,1)
    - Historically popular since they have nice interpretation as a saturating ‚Äúfiring rate‚Äù (ÊîæÁîµÁéá) of a neuron
    - Problems
        - Saturated neurons ‚Äúkill‚Äù the gradients (Ê¢ØÂ∫¶Ê∂àÂ§±)
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2054.png)
            
        - Sigmoid outputs are not zero-centered
            - Consider what happens when the input to a neuron (x) is always positive, what can we say about the gradients on w?
            Always all positive or all negative :(
            (this is also why you want zero-mean data!)
                
                ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2055.png)
                
        - exp() is a bit compute expensive
- tanh
    - Squashes numbers to range [-1,1]
    - zero centered (nice)
    - still kills gradients when saturated :(
- ReLU (Rectified Linear Unit)
    
    Computes $f(x) = max(0,x)$
    
    - Advantages
        - Does not saturate (È•±Âíå) (in +region)
        - Very computationally efficient
        - Converges much faster than sigmoid/tanh in practice (e.g. 6x)
        - Actually more biologically plausible than sigmoid
    - Disadvantages
        - Not zero-centered output
        - An annoyance:  what is the gradient when x < 0? ‚áí 0
            - Dead ReLU
                
                ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2056.png)
                
                - ËøôÂº†ÂõæÁöÑËß£Èáä
                    
                    ÂØπ‰∫é‰∏çÂêåÁöÑÊùÉÈáçÂØπÂ∫î‰∏çÂêåÁöÑÂàÜÂâ≤Ë∂ÖÂπ≥Èù¢ÔºåÊï∞ÊçÆÁªèËøáÂä†ÊùÉÂêéËæìÂÖ• ReLUÔºåÈÄöËøáË∂ÖÂπ≥Èù¢Â∞ÜÁ©∫Èó¥ÂàÜÂâ≤‰∏∫0ÂíåÈùû0Âå∫ÂüüÔºå‰∏çÂêåÁöÑRELUËÆ≠ÁªÉÂæóÂà∞ÁöÑÂàÜÂâ≤Ë∂ÖÂπ≥Èù¢‰πüÁõ∏Â∫î‰∏çÂêå„ÄÇÂÅáËÆæËæìÂÖ•Êï∞ÊçÆ‰∏∫‰∫åÁª¥ÔºåËøôÈáåÁî±‰∫éÈöèÊú∫ÂàùÂßãÂåñw‰∏îÊØè‰∏™wÂêëÈáèÂîØ‰∏ÄÂØπÂ∫î‰∫åÁª¥Âπ≥Èù¢ÂÜÖÁöÑ‰∏ÄÊù°Áõ¥Á∫øÔºåÂêåÊó∂‰πüÂèØ‰ª£Ë°®‰∏Ä‰∏™Á•ûÁªèÂÖÉÔºàÁõ¥Á∫ø‰∏äÊñπÁöÑÂå∫ÂüüË°®Á§∫ËØ•Á•ûÁªèÂÖÉÁöÑËæìÂá∫‰∏∫Ê≠£Ôºå‰∏ãÊñπ‰∏∫Ë¥üÔºåËÄå‰∏∫Ë¥üÁöÑ‰ºöÂØºËá¥Âõû‰º†Ê¢ØÂ∫¶‰∏∫0ÔºåÂç≥Á•ûÁªèÂÖÉÊó†Ê≥ïÊõ¥Êñ∞Ôºâ„ÄÇÂΩìÊï∞ÊçÆ‰∫ëÔºàÂç≥ÊâÄÊúâËæìÂÖ•Ê†∑Êú¨ÊûÑÊàêÁöÑÈõÜÂêàÔºâÂÖ®ÈÉ®Â§Ñ‰∫éÊüê‰∏™Á•ûÁªèÂÖÉÂØπÂ∫îÁõ¥Á∫øÁöÑ‰∏ãÊñπÊó∂ÔºåÂ∞ÜÂØºËá¥Êó†ËÆ∫ËæìÂÖ•Âì™‰∏™Ê†∑Êú¨ÈÉΩÊó†Ê≥ï‰ΩøÁ•ûÁªèÂÖÉÁöÑËæìÂá∫‰∏∫Ê≠£ÔºåÂõ†Ê≠§ËØ•Á•ûÁªèÂÖÉÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠Â∞Ü‰∏ÄÁõ¥Êó†Ê≥ïÊõ¥Êñ∞ÔºåÊâÄ‰ª•ËØ¥dead‰∫Ü
                    
                    people like to initialize ReLU neurons with slightly positive biases (e.g. 0.01) in order to increase the likelihood of it being active at initialization and to get some updates.
                    
        
        Reasons:
        
        - when we have bad initialization
        - when the learning rate is too high
- Leaky ReLU
    
    $f(x) = max(0.01x,x)$
    
    - Advantages:
        - Doesn‚Äôt have any saturating (‰∏ç‰ºöÈ•±Âíå)
        - Computationally efficient
        - Converges much faster than sigmoid/tanh in practice! (e.g. 6x)
        - will not ‚Äúdie‚Äù.
    
    Parametric Rectifier (PReLU)
    
    $f(x) = max(\alpha x,x)$
    backprop into $\alpha$ (parameter)
    
    more flexible
    
- ELU (Exponential Linear Units)
    - Advantages
        - All benefits of ReLU
        - Closer to zero mean outputs
        - Negative saturation regime compared with Leaky ReLU adds some robustness to noise
    - Disadvantage
        - Computation requires exp()
- Maxout ‚ÄúNeuron‚Äù
    
    $\max(w_1^T x + b_1, w_2^T x + b_2)$
    
    - Does not have the basic form of dot product ‚Üí nonlinearity
    - Advantages
        - Generalizes ReLU and Leaky ReLU
        - Linear Regime! Does not saturate! Does not die!
        - Problem: doubles the number of parameters/neuron :(
- In practice:
    - Use ReLU. Be careful with your learning rates
    - Try out Leaky ReLU / Maxout / ELU
    - Try out tanh but don‚Äôt expect much
    - Don‚Äôt use sigmoid

### Data Preprocessing

If the input to some layer of the neural network is not centered or normalized, small perturbations (perturbations) in the weight matrix of that layer could cause large perturbations in the output of that layer, which make learning difficult.

- Example
    
    for a binary classification problem
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2057.png)
    

In practice for Images: center only
Not common to normalize variance, to do PCA or whitening

e.g. consider CIFAR-10 example with [32,32,3] images

- Subtract the mean image (e.g. AlexNet) (mean image = [32,32,3] array)
- Subtract per-channel mean (e.g. VGGNet) (mean along each channel = 3 numbers)

![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2058.png)

![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2059.png)

### Weight Initialization

Q: What happens when W=0 init is used?

A: All the neurons will do the same thing ‚Üí output the same thing, getting the same gradient ‚Üí update in the same way

- First idea: Small random numbers
    
    W = 0.01 * np.random.randn(D , H) (gaussian with zero mean and 1e-2 standard deviation)
    
    Initialization too small: Activations go to zero, gradients will also be zero ‚Üí no learning
    
    - Works ~okay for small networks, but problems with deeper networks.
    Q: think about the backward pass. What do the gradients look like?
    Hint: think about backward pass for a W*X gate. ‚Üí X
    - Example: 10-layer net with 500 neurons on each layer, using tanh non-linearities, and initializing as described above.
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2060.png)
        
- Let weights be all big
    
    Almost all neurons completely saturated (È•±Âíå), either -1 and 1. Gradients will be all zero, no learning
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2061.png)
    
- Xavier initialization
    
    Reasonable initialization. (Mathematical derivation assumes linear activations)
    
    W = np.random.randn(fan_in, fan_out) / np.sgrt(fan_in) # layer initialization
    
    ÊøÄÊ¥ªÂÄºÁöÑÊñπÂ∑ÆÊòØÈÄêÂ±ÇÈÄíÂáèÁöÑÔºåËøôÂØºËá¥ÂèçÂêë‰º†Êí≠‰∏≠ÁöÑÊ¢ØÂ∫¶‰πüÈÄêÂ±ÇÈÄíÂáè„ÄÇË¶ÅËß£ÂÜ≥Ê¢ØÂ∫¶Ê∂àÂ§±ÔºåÂ∞±Ë¶ÅÈÅøÂÖçÊøÄÊ¥ªÂÄºÊñπÂ∑ÆÁöÑË°∞ÂáèÔºåÊúÄÁêÜÊÉ≥ÁöÑÊÉÖÂÜµÊòØÔºåÊØèÂ±ÇÁöÑËæìÂá∫ÂÄºÔºàÊøÄÊ¥ªÂÄºÔºâ‰øùÊåÅÈ´òÊñØÂàÜÂ∏É„ÄÇ
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2062.png)
    
    However, when using the ReLU nonlinearity it breaks.
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2063.png)
    

### Batch Normalization

(ÊâπÈáèÂΩí‰∏ÄÂåñ)

- Intro
    
    One way to make deep networks easier to train is to use more sophisticated optimization procedures such as SGD+momentum, RMSProp, or Adam. Another strategy is to change the architecture of the network to make it easier to train. One idea along these lines is batch normalization, proposed by [1] in 2015.
    
    To understand the goal of batch normalization, it is important to first recognize that **machine learning methods tend to perform better with input data consisting of uncorrelated features with zero mean and unit variance**. When training a neural network, we can preprocess the data before feeding it to the network to explicitly decorrelate its features. This will ensure that the first layer of the network sees data that follows a nice distribution. However, even if we preprocess the input data, the activations at deeper layers of the network will likely no longer be decorrelated and will no longer have zero mean or unit variance, since they are output from earlier layers in the network. Even worse, during the training process the distribution of features at each layer of the network will shift as the weights of each layer are updated.
    
    The authors of [1] hypothesize that the shifting distribution of features inside deep neural networks may make training deep networks more difficult. To overcome this problem, they propose to insert into the network layers that normalize batches. At training time, such a layer uses a minibatch of data to estimate the mean and standard deviation of each feature. These estimated means and standard deviations are then used to center and normalize the features of the minibatch. A running average of these means and standard deviations is kept during training, and at test time these running averages are used to center and normalize features.
    
    It is possible that this normalization strategy could reduce the representational power of the network, since it may sometimes be optimal for certain layers to have features that are not zero-mean or unit variance. To this end, the batch normalization layer includes learnable shift and scale parameters for each feature dimension.
    
    [1] [[Sergey Ioffe and Christian Szegedy, "Batch Normalization: Accelerating Deep Network Training by Reducing
    Internal Covariate Shift", ICML 2015.]](https://arxiv.org/abs/1502.03167)
    

consider a batch of activations at some layer. To make each dimension unit gaussian, apply: 

suppose we have $N$ training examples in the current batch and each example has dimension $D$

Steps

- compute the empirical mean and variance independently for each dimension (each feature element).
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2064.png)
    
- Normalize:
$\hat{x}^{(k)} = \frac{x^{(k)} - E[x^{(k)}]}{\sqrt{Var[x^{(k)}]}}$
this is a vanilla differentiable function

Usually inserted **after** Fully Connected or Convolutional layers, and **before** nonlinearity.

![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2065.png)

- Algorithm:
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2066.png)
    
    Note, the network can learn $\gamma(k), \beta(k)$ to get the flexibility to slightly scaled of shifted
    
    - Intuition
        
        Êàë‰ª¨ÂºïÂÖ•‰∏Ä‰∫õ batch normalization ÁöÑÂÖ¨Âºè. Ëøô‰∏âÊ≠•Â∞±ÊòØÊàë‰ª¨Âú®ÂàöÂàö‰∏ÄÁõ¥ËØ¥ÁöÑ normalization Â∑•Â∫èÔºå‰ΩÜÊòØÂÖ¨ÂºèÁöÑÂêéÈù¢ËøòÊúâ‰∏Ä‰∏™ÂèçÂêëÊìç‰Ωú, Â∞Ü normalize ÂêéÁöÑÊï∞ÊçÆÂÜçÊâ©Â±ïÂíåÂπ≥Áßª. ÂéüÊù•ËøôÊòØ‰∏∫‰∫ÜËÆ©Á•ûÁªèÁΩëÁªúËá™Â∑±ÂéªÂ≠¶ÁùÄ‰ΩøÁî®Âíå‰øÆÊîπËøô‰∏™Êâ©Â±ïÂèÇÊï∞ gammaÔºåÂíå Âπ≥ÁßªÂèÇÊï∞ Œ≤, ËøôÊ†∑Á•ûÁªèÁΩëÁªúÂ∞±ËÉΩËá™Â∑±ÊÖ¢ÊÖ¢Áê¢Á£®Âá∫ÂâçÈù¢ÁöÑ normalization Êìç‰ΩúÂà∞Â∫ïÊúâÊ≤°ÊúâËµ∑Âà∞‰ºòÂåñÁöÑ‰ΩúÁî®ÔºåÂ¶ÇÊûúÊ≤°ÊúâËµ∑Âà∞‰ΩúÁî®ÔºåÊàëÂ∞±‰ΩøÁî® gamma Âíå belt Êù•ÊäµÊ∂à‰∏Ä‰∫õ normalization ÁöÑÊìç‰Ωú.
        
        ‰æãÂ¶ÇÂú® tanh ÊøÄÊ¥ªÂáΩÊï∞‰∏≠ÔºåÊàë‰ª¨Â∏åÊúõÊéßÂà∂È•±ÂíåÁöÑÁ®ãÂ∫¶ÔºåËÄå‰∏çÊòØÂÆåÂÖ®‰∏çÈ•±Âíå
        
    - gradient
        
        NB ÂÖ¨ÂºèÔºö $\hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma^2_B + \varepsilon}},y_i = \gamma \hat{x}_i + \beta$
        
        computational graph:
        
        $$
        \begin{align*}                      &             & \gamma    &             & x_i        & \rightarrow & \rightarrow                     \\                      & \nearrow    &           & \nearrow    &            &             &             & \searrow          \\    L \rightarrow y_i & \rightarrow & \hat{x_i} & \rightarrow & \mu_B      & \rightarrow & \mu_B       & \rightarrow & x_i \\                      & \searrow    &           & \searrow    &            & \nearrow                                      \\
                              &             & \beta     &             & \sigma^2_B &                                               \\\end{align*}
        $$
        
        - ÂØπ‰∫éÁº©ÊîæÂèÇÊï∞ $\gamma$ ÁöÑÊ¢ØÂ∫¶Ôºö
        $\frac{\partial L}{\partial \gamma} = \sum\limits_{i=1}^{m}\frac{\partial L}{\partial y_i} \frac{\partial y_i}{\partial \gamma} =\sum\limits_{i=1}^{m} \frac{\partial L}{\partial y_i} \cdot \hat{x}_i$
        - ÂØπ‰∫éÂÅèÁßªÂèÇÊï∞ $\beta$ ÁöÑÊ¢ØÂ∫¶Ôºö 
        $\frac{\partial L}{\partial \beta} = \sum\limits_{i=1}^{m}\frac{\partial L}{\partial y_i} \frac{\partial y_i}{\partial \beta} =\sum\limits_{i=1}^{m} \frac{\partial L}{\partial y_i}$
        - ÂØπ‰∫é $\hat{x_i}$ ÁöÑÊ¢ØÂ∫¶ $\frac{\partial L}{\partial \hat{x_i}} = \frac{\partial L}{\partial y_i} \cdot \frac{\partial y_i}{\partial \hat{x}_i}  = \frac{\partial L}{\partial y_i} \cdot \gamma$
        - ÂØπ‰∫é $\sigma^2_B$ ÁöÑÊ¢ØÂ∫¶Ôºö $\frac{\partial L}{\partial \sigma^2_B} = \sum\limits_{i=1}^{m} \frac{\partial L}{\partial y_i} \cdot \frac{\partial y_i}{\partial \hat{x}_i} \cdot \frac{\partial \hat{x}i}{\partial \sigma^2_B} = \sum\limits_{i=1}^{m} \frac{\partial L}{\partial y_i} \cdot \gamma \cdot \left(x_i - \mu_B\right) \cdot \left[-\frac{1}{2} (\sigma^2_B + \epsilon)^{-\frac{3}{2}}\right]$
        Ê≥®ÊÑèÊòØ $\sigma^2_B$ ËÄå‰∏çÊòØ $\sigma_B$
        - ÂØπ‰∫é $\mu_B$ ÁöÑÊ¢ØÂ∫¶Ôºö $\frac{\partial L}{\partial \mu_B} = \sum\limits_{i=1}^{m} \frac{\partial L}{\partial y_i} \cdot \frac{\partial y_i}{\partial \hat{x}_i} \cdot \frac{\partial \hat{x}i}{\partial \mu_B} = \sum\limits_{i=1}^{m} \frac{\partial L}{\partial \hat{x}_i}   \cdot \left(-\frac{1}{\sqrt{\sigma^2_B + \epsilon}}\right) + \frac{\partial L}{\partial \sigma^2_B} \cdot \left(-\frac{2}{m} \sum\limits_{i=1}^{m}(x_i - \mu_B)\right)$
        - ÂØπ‰∫é $x_i$ ÁöÑÊ¢ØÂ∫¶Ôºö $\frac{\partial L}{\partial x_i} = \frac{\partial L}{\partial \hat{x}_i} \cdot \frac{1}{\sqrt{\sigma^2_B + \varepsilon}} + \frac{\partial L}{\partial \sigma^2_B} \cdot \frac{2}{m}(x_i - \mu_B) + \frac{\partial L}{\partial \mu_B} \cdot \frac{1}{m}$
- Advantages
    - Improves gradient flow through the network
    - Allows higher learning rates
    - Reduces the strong dependence on initialization
    - Acts as a form of regularization in a funny way, and slightly reduces the need for dropout, maybe
- Note:
    
    at **test time** BatchNorm layer functions differently:
    The mean/std are not computed based on the batch. Instead, a single fixed empirical mean of activations during training is used. (e.g. can be estimated during training with running averages)
    
    Â∞ÜÂú®ËÆ≠ÁªÉÈõÜ‰∏≠ËÆ°ÁÆóÂæóÂà∞ÁöÑÂùáÂÄºÊñπÂ∑ÆÂ∫îÁî®Âà∞‰πãÂêéÊâÄÊúâÁöÑÊï∞ÊçÆ‰∏≠ÔºåÂåÖÊã¨ÊµãËØïÈõÜ„ÄÅÈ™åËØÅÈõÜÔºåÁõÆÁöÑÊòØÁªô‰∫àÊâÄÊúâÊï∞ÊçÆÁõ∏ÂêåÁöÑÊ†áÂáÜÂåñÂäõÂ∫¶
    
- Layer Normalization
    
    Batch normalization has proved to be effective in making networks easier to train, but the dependency on batch size makes it less useful in complex networks which have a cap on the input batch size due to hardware limitations. 
    
    Several alternatives to batch normalization have been proposed to mitigate this problem; one such technique is Layer Normalization [2]. Instead of normalizing over the batch, we normalize over the features. In other words, when using Layer Normalization, each feature vector corresponding to a single datapoint is normalized based on the sum of all terms within that feature vector.
    
    [2] [Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. "Layer Normalization." stat 1050 (2016): 21.](https://arxiv.org/pdf/1607.06450.pdf)
    
- Spatial Batch Normalization
    
    We already saw that batch normalization is a very useful technique for training deep fully connected networks. As proposed in the original paper (link in `BatchNormalization.ipynb`), batch normalization can also be used for convolutional networks, but we need to tweak it a bit; the modification will be called "spatial batch normalization."
    
    Normally, batch-normalization accepts inputs of shape `(N, D)` and produces outputs of shape `(N, D)`, where we normalize across the minibatch dimension `N`. For data coming from convolutional layers, batch normalization needs to accept inputs of shape `(N, C, H, W)` and produce outputs of shape `(N, C, H, W)` where the `N` dimension gives the minibatch size and the `(H, W)` dimensions give the spatial size of the feature map.
    
    If the feature map was produced using convolutions, then **we expect every feature channel's statistics e.g. mean, variance to be relatively consistent both between different images, and different locations within the same image** -- after all, every feature channel is produced by the same convolutional filter! Therefore, spatial batch normalization computes a mean and variance for each of the `C` feature channels by computing statistics over the minibatch dimension `N` as well the spatial dimensions `H` and `W`.
    
    [1] [Sergey Ioffe and Christian Szegedy, "Batch Normalization: Accelerating Deep Network Training by Reducing
    Internal Covariate Shift", ICML 2015.](https://arxiv.org/abs/1502.03167)
    

### Babysitting the Learning Process

(ËßÇÂØüÂ≠¶‰π†ËøáÁ®ã)

- Double check that the loss is reasonable:
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2067.png)
    
- Try to train
    - Make sure that you can overfit very small portion of the training data
        - take the first 20 examples from CIFAR-10
        - turn off regularization (reg = 0.0)
        - use simple vanilla ‚Äòsgd‚Äô
    - Start with small regularization and find learning rate that makes the loss go down.
        - Loss barely changing: Learning rate is probably too low
        - cost: NaN almost always means high learning rate‚Ä¶
        - Rough range for learning rate we should be cross-validating is [1e-3 ‚Ä¶ 1e-5]

### Hyperparameter Optimization

- Hyperparameters:
    - network architecture
    - learning rate, its decay schedule, update type
    - regularization (L2/Dropout strength)
- Cross-validation strategy
    
    coarse -> fine cross-validation in stages
    
    - First stage: only a few epochs to get rough idea of what params work
    - Second stage: longer running time, finer search
    - ‚Ä¶ (repeat as necessary)
    
    Tip for detecting explosions in the solver: If the cost is ever > 3 * original cost, break out early
    
    - example
        - run coarse search  for 5 epochs
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2068.png)
            
        - Now run finer search
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2069.png)
            
- Random Search vs. Grid Search
    
    ÈöèÊú∫ÂèñÂÄºÂèØ‰ª•Âú®ÊØè‰∏™Áª¥Â∫¶Ë¶ÜÁõñÊõ¥Â§öÁöÑÂÄº
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2070.png)
    
- Monitor and visualize the loss curve
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2071.png)
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2072.png)
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2073.png)
    

## Lecture 7 Training Neural Networks Part 2

### Parameter update schemes

- Problems with SGD
    
    SGD:
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2074.png)
    
    - One problem: poorly conditioned coordinate
        
        What if loss changes quickly in one direction and slowly in another? What does gradient descent do?
        Very slow progress along shallow dimension, jitter along steep direction
        
        Because the direction of the gradient does not align with the direction towards the minima 
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2075.png)
        
        It‚Äôs much more common in high dimension that the ratio is very large 
        
    - Another problem: local minima and saddle points
        
        What if the loss function has a local minima or saddle point (ÈûçÁÇπ)?
        ‚áí Zero gradient, gradient descent gets stuck
        
        Saddle points much more common in high dimension
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2076.png)
        
    - Another problem: ‚ÄòS‚Äô
        
        Our gradients come from mini-batches so they can be noisy!
        
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2077.png)
    
- SGD + Momentum
    - Build up ‚Äú**velocity**‚Äù as a running mean of gradients
    - hyperparameter $\rho$  gives ‚Äúfriction‚Äù, typically $\rho$ = 0.9 or 0.99
    - We step in the direction of our velocity vector rather than the direction of the raw gradient vector
    - Intuitively, the velocity is kind of a weighted sum of the gradients over time, with more recent gradients being weighted heavier
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2078.png)
        
        result:
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2079.png)
        
    - Nesterov Momentum
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2080.png)
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2081.png)
        
        result:
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2082.png)
        
        because of the correction factor in Nesterov, it‚Äôs not overshooting (Ë∂ÖË∞É) quite as drastically compared to vanilla momentum
        
- AdaGrad
    
    Added element-wise scaling of the gradient based on the historical sum of squares in each dimension
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2083.png)
    
    1e-7: to make sure we‚Äôre not dividing by 0
    
    - Q1: What happens with AdaGrad?
        
        A: Suppose we have two coordinates, one that always has a very high gradient and one that always has a verysmall gradient
        
        when we add the sum of the squares of the small gradient, we're going to be dividing by a small number, so we'l accelerate movement along the slow dimension
        
        along the other dimension, where the gradients tend to be very large, so we‚Äôll slow down the progress along the wiggling dimension
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2084.png)
        
    - Q2: What happens to the step size over long time?
        
        in the convex case, there's some really nice theory showing that this is actually really good case in the convex case, as you approach a minimum, you kind of want to slow down so you actually converge
        
        But in the non-convex case, that's a little bit problematic because as you come towards a saddle point, you might get stuck with AdaGrad, and then you kind of no longer make any progress.
        
    - A variation of AdaGrad: RMSProp
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2085.png)
        
        decay_rate: commonly 0.9 or 0.99
        
        result:
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2086.png)
        
- Adam
    - approximately
        
        Adam is sort of RMSProp with momentum
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2087.png)
        
        Q: What happens at first timestep?
        
        A: second_moment is very close to 0, which will make a very large step at the beginning
        
    - full form
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2088.png)
        
        we create unbiased estimate (Êó†ÂÅè‰º∞ËÆ°) of the two moments by incorporating the current time step $t$, and we update use these unbiased estimate rather than the original moments.
        
        Adam with $\beta_1 = 0.9$, $\beta_2 = 0.999$, and learning_rate = 1e-3 or 5e-4 is a great starting point for many models!
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2089.png)
        

### Learning rate schedules

SGD, SGD+Momentum, Adagrad, RMSProp, Adam all have **learning rate** as a hyperparameter.

Let Learning rate decay over time!

- step decay:
e.g. decay learning rate by half every few epochs.
- exponential decay (ÊåáÊï∞Ë°∞Âáè):
$\alpha = \alpha_0 e^{-kt}$
- 1/t decay:
$\alpha = \frac{\alpha_0}{(1 + kt)}$

![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2090.png)

imagine that the model got near some good region and the gradients is small, so it‚Äôs kind of bouncing around too much. If we drop the learning rate, it lets it continue to make progress

Learning rate decay is kind of a second-order hyperparameter, shouldn‚Äôt be optimized from the start. We should pick a good learning rate with no learning rate decay at the beginning, and trying to cross-validate jointly over learning rate decay and initial learning rate.

### Second-Order Optimization

- First-Order Optimization
    
    ‰∏äËø∞Êàë‰ª¨ËÆ≤ÁöÑÈÉΩÂ±û‰∫éËøôÁßç
    
    Ê±Ç‰∏ÄÈò∂ÂæÆÂàÜÔºåÁî®Áõ¥Á∫øËøë‰ºº Loss Êõ≤Á∫øÔºåÂπ∂ÊúùÂâçËøà‰∏ÄÂ∞èÊ≠•
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2091.png)
    
- Second-Order Optimization:
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2092.png)
    
    second-order Taylor expansion (È´òÁª¥ÊÉÖÂÜµ‰∏ãÂØπÁõÆÊ†áÂáΩÊï∞ $J(\theta)$ ËøõË°å‰∫åÈò∂Ê≥∞ÂãíÂ±ïÂºÄ):
    
    $J(\theta) \approx J(\theta_0) + (\theta - \theta_0)^T\nabla_{\theta}J(\theta_0) + \frac{1}{2}(\theta - \theta_0)^TH(\theta - \theta_0)$
    
    Solving for the critical point we obtain the Newton parameter update:
    
    $\theta^* = \theta_0 - H^{-1}\nabla_{\theta}J(\theta_0)$
    
    - Hessian Matrix
        
        $H$ ‰ª£Ë°®Êµ∑Ê£ÆÁü©ÈòµÔºàHessian MatrixÔºâÔºåÂÆÉÊòØÁõÆÊ†áÂáΩÊï∞ $J(\theta)$ ÂÖ≥‰∫éÂèÇÊï∞ $\theta$ ÁöÑ‰∫åÈò∂ÂÅèÂØºÊï∞ÁªÑÊàêÁöÑÁü©Èòµ„ÄÇÂÖ∑‰ΩìÂú∞ÔºåÂØπ‰∫é‰∏Ä‰∏™Â§öÂÖÉÂáΩÊï∞ $J(\theta_1, \theta_2, ..., \theta_n)$ÔºåÂÖ∂Êµ∑Ê£ÆÁü©Èòµ $H$ ÂÆö‰πâ‰∏∫Ôºö$H_{ij} = \frac{\partial^2 }{\partial \theta_i \partial \theta_j}J(\theta)$
        ËøôÈáåÁöÑ $i$ Âíå $j$ ÂàÜÂà´Ë°®Á§∫ÂèÇÊï∞ÁöÑ‰∏ãÊ†áÔºåÂèñÂÄºËåÉÂõ¥‰ªé 1 Âà∞ $n$„ÄÇÊØè‰∏™ÂÖÉÁ¥† $H_{ij}$ Ë°®Á§∫ÂáΩÊï∞Âú®ÁÇπ $\theta$ Â§ÑÂÖ≥‰∫éÁ¨¨ $i$ ‰∏™ÂèÇÊï∞ÂíåÁ¨¨ $j$ ‰∏™ÂèÇÊï∞ÁöÑÊ∑∑ÂêàÂÅèÂØºÊï∞„ÄÇ
        
    
    step directly to the minimum of this quadratic approximationn (‰∫åÊ¨°ÈÄºËøë) to tour function ‚Üí **No Learning rate**
    
    - Áº∫ÁÇπÔºöËÆ°ÁÆóÈáèÂ§™Â§ß
        
        Hessian has $O(N^2)$ elements, Inverting takes $O(N^3)$
        
    - ÊîπËøõÔºö
        - Quasi-Newton methods (BGFS most popular):
        instead of inverting the Hessian (O(n^3)), approximate
        inverse Hessian with rank 1 updates over time ($O(n^2)$
        each).
        - L-BFGS (Limited memory BFGS):
        Does not form/store the full inverse Hessian.
    - In practice
        - **Adam** is a good default choice in most cases
        - If you can afford to do full batch updates then try out L-BFGS (and don‚Äôt forget to disable all sources of noise)

### Model Ensembles

(Ê®°ÂûãËûçÂêà)

- One way
    - Train multiple independent models (can have different hyperparameter)
    - At test time average their results
    - Enjoy 2% extra performance
- Another way
    - Instead of training independent models, use multiple **snapshots** of a single model during training!
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2093.png)
    
- Another way
    - Instead of using actual parameter vector, keep a moving average of the parameter vector and use that at test time (Polyak averaging)
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2094.png)
    

### Regularization

(Ê≠£ÂàôÂåñ)

How to improve single-model performance? Use **regularization**

- Dropout
    
    In each forward pass, randomly set some activations of neurons to zero
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2095.png)
    
    The probability of dropping is a hyperparameter, 0.5 is common
    
    Commonly used in fully connected layer
    
    - Implement
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2096.png)
        
    - interpretation
        - Forces the network to have a redundant representation (ÂÜó‰Ωô), to distribute its idea across different features
        Prevents co-adaptation (Áõ∏‰∫íÈÄÇÂ∫î) of features to help prevent overfitting
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2097.png)
            
        - Dropout is like training a large ensemble of models (that share parameters) within a single model.
            - Each binary mask is one model
    - Dropout at Test time
        
         Dropout makes our output random,so we want to ‚Äú**average out**‚Äù the randomness at test-time
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2098.png)
        
        $y = f(x) = E_z \left[ f(x, z) \right] = \int p(z) f(x, z) dz$
        
        - approximate the integral
            
            Consider a single neuron:
            
            - At test time we have: $E[a] = w_1 x + w_2 y$
            - During training we have: $E[a] = \frac{1}{4}(w_1 x + w_2 y) + \frac{1}{4}(w_1 x + 0 y) + \frac{1}{4}(0 x + 0 y) + \frac{1}{4}(0 x + w_2 y) = \frac{1}{2}(w_1 x + w_2 y)$
            
            **At test time, multiply by dropout probability**
            
    - Summary
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%2099.png)
        
        More common: ‚ÄúInverted dropout‚Äù
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20100.png)
        
- More general strategy
    
    Strategy
    
    - Training: Add some kind of randomness to the network to prevent from over-fitting 
    $y = f_W(x, z)$
    - Testing: Average out randomness (sometimes approximate) to improve generalization
    $y = f(x) = E_z \left[ f(x, z) \right] = \int p(z) f(x, z) dz$
    - Example
        - Dropout
        - Batch Normalization
            - Training: Normalize using stats from random minibatches
            - Testing: Use fixed stats to normalize
        - Data Augmentation
            - Random mix/combinations of :
                - translation
                - rotation
                - stretching
                - shearing
                - lens distortions
                - ‚Ä¶
        - DropConnect
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20101.png)
            
        - Fractional Max Pooling (ÈÉ®ÂàÜÊúÄÂ§ßÊ±†Âåñ)
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20102.png)
            
        - Stochastic Depth
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20103.png)
            

### Transfer learning / fine-tuning

Somtime we overfit just because we don‚Äôt have enough data, transfer learning is another way to combat that (one way is regularization).

![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20104.png)

![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20105.png)

Transfer learning with CNNs is pervasive (it‚Äôs the norm, not an exception)

![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20106.png)

Takeaway for your projects and beyond:
Have some dataset of interest but it has < ~1M images?

- Find a very large dataset that has similar data, train a big ConvNet there
- or transfer learn to your dataset
Deep learning frameworks provide a ‚ÄúModel Zoo‚Äù of pretrained models so you don‚Äôt need to train your own
Caffe: [https://github.com/BVLC/caffe/wiki/Model-Zoo](https://github.com/BVLC/caffe/wiki/Model-Zoo)
TensorFlow: [https://github.com/tensorflow/models](https://github.com/tensorflow/models)
PyTorch: [https://github.com/pytorch/vision](https://github.com/pytorch/vision)

## Lecture 8  Deep Learning Software

### CPU vs GPU

- CPU: Fewer cores, but each core is much faster and much more capable; great at sequential tasks
- GPU: More cores, but each core is much slower and ‚Äúdumber‚Äù; great for parallel tasks
- Example: Matrix Multiplication
    
    for massively parallel problems, GPUs do much better than CPUs
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20107.png)
    
- Example: Convolution
    
    we have an input tensor and weight tensor, at every point in the output tensor after a convolution is again some inner produet between some part of the weights and some part of the input.
    
    A GPU could paralyze this conputation, split it all up across the many cores and compute it very quickly.
    
- Programming GPUs
    - CUDA (NVIDIA only)
        - Write C-like code that runs directly on the GPU
        - Higher-level APIs: cuBLAS, cuFFT, cuDNN, etc
    - OpenCL
        - Similar to CUDA, but runs on anything
        - Usually slower :(
    - Udacity: Intro to Parallel Programming
    [https://www.udacity.com/course/cs344](https://www.udacity.com/course/cs344)
    - For deep learning just use existing libraries, we don‚Äôt need to write CUDA code from scratch by ourselves
- CPU / GPU Communication
    
    If you aren‚Äôt careful, training can bottleneck (Áì∂È¢à) on reading data and transferring to GPU!
    Solutions:
    
    - Read all data into RAM
    - Use SSD instead of HDD
    - Use multiple CPU threads to prefetch data

### Deep Learning Frameworks

![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20108.png)

- The point of deep learning frameworks
    - Easily build big computational graphs
    - Easily compute gradients in computational graphs
    - Run it all efficiently on GPU (wrap cuDNN, cuBLAS, etc)
- Example
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20109.png)
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20110.png)
    
    - Problems when written in Numpy:
        - Can‚Äôt run on GPU, Numpy is CPU only
        - Have to compute our own gradients
    
    In the forward pass, the TensorFlow and PyTorch code look almost like Numpy because Numpy has a beautiful API. 
    
    But we can compute gradients automatically and we can run on the GPU by using TensorFlow or PyTorch
    

### TensorFlow

[TensorFlow.pdf](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/TensorFlow.pdf)

### Pytorch

[PyTorch.pdf](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/PyTorch.pdf)

### **Static vs Dynamic Graphs**

[Static vs Dynamic Graphs.pdf](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Static_vs_Dynamic_Graphs.pdf)

### Caffe

[Caffe.pdf](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Caffe.pdf)

- **Advice**
    - **TensorFlow** is a safe bet for most projects. Not perfect but has huge community, wide usage. Maybe pair with high-level wrapper (Keras, Sonnet, etc)
    - I think **PyTorch** is best for research. However still new, there can be rough patches.
    - Use **TensorFlow** for one graph over many machines
    - Consider **Caffe**, **Caffe2,** or **TensorFlow** for production deployment
    - Consider **TensorFlow** or **Caffe2** for mobile

## Lecture 9 CNN Architectures

[Lecture 9 CNN Architectures.pdf](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Lecture_9_CNN_Architectures.pdf)

## Lecture 10 Recurrent Neural Nwetworks

### Intro

- Recurrent Neural Networks: Process Sequences
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20111.png)
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20112.png)
    
    Notice: the same function and the same set of parameters $W$ are used at every time step.
    
    - (Vanilla) Recurrent Neural Network
        
        The state consists of a single ‚Äúhidden‚Äù vector $h$
        
        $h_t = f_W(h_{t-1}, x_t)$ ‚Üí
        $h_t = \tanh(W_h h_{t-1} + W_x x_t+b)$
        $y_t = W_y h_t$
        
        - $tanh$ ÁöÑÂØºÊï∞
            
            $\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$
            
            $\frac{d}{dx} \tanh(x) = \frac{\left(e^x + e^{-x}\right)\left(e^x + e^{-x}\right) - \left(e^x - e^{-x}\right)\left(e^x - e^{-x}\right)}{\left(e^x + e^{-x}\right)^2}
            = 1 - \frac{\left(e^x - e^{-x}\right)^2}{\left(e^x + e^{-x}\right)^2} = 1 - \tanh^2(x)$
            
    - Computational Graph
        - Many to Many
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20113.png)
            
        - Many to One
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20114.png)
            
        - One to Many
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20115.png)
            
        - Sequence to Sequence: Many-to-one + one-to-many
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20116.png)
            
    - Example: Character-level Language Model
        
        Vocabulary: [h,e,l,o] Example training sequence: ‚Äúhello‚Äù
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20117.png)
        
        At test-time sample characters one at a time, feed back to model
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20118.png)
        
        Q: Why sample instead of just taking the character with the largest score?
        
        A: Can get more diversity in the outputs.
        
    - backprop
        - Backpropagation through time
            - Forward through entire sequence to compute loss, then backward through entire sequence to compute gradient
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20119.png)
            
        - Truncated Backpropagation through time
            - Run forward and backward through chunks of the sequence instead of whole sequence
            - Carry hidden states forward in time forever, but only backpropagate for some smaller number of steps
            - Is a way to approximate the gradients without going making a backwards pass through your potentially very large sequence of data
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20120.png)
            
    - Python implement
        
        [https://gist.github.com/karpathy/d4dee566867f8291f086](https://gist.github.com/karpathy/d4dee566867f8291f086)
        
    - Multilayer RNNs
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20121.png)
        
    - Vanilla RNN Gradient Flow
        
        $$
        \begin{align*}
        h_t &= \tanh(W_{hh} h_{t-1} + W_{xh} x_t) \\
        &= \tanh \left( \begin{pmatrix} W_{hh} & W_{hx}\end{pmatrix} \begin{pmatrix} h_{t-1} \\ x_t \end{pmatrix} \right) \\
        &= \tanh \left( W \begin{pmatrix} h_{t-1} \\ x_t \end{pmatrix} \right)
        \end{align*}
        $$
        
        Backpropagation from $h_t$ to $h_{t-1}$ multiplies by $W$ (actually $W^T_{hh}$)
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20122.png)
        
        Computing gradient of $h_0$ involves many factors of $W$ (and repeated tanh)
        
        - Largest singular value of $W$ > 1: Exploding gradients
        - Largest singular value of $W$ < 1: Vanishing gradients
        
        ÁêÜËß£ÔºöÊØîÂ¶Ç $W$ ÊòØ‰∏Ä‰∏™Êï∞ÔºåÂ¶ÇÊûú $|W|>1$ÔºåÂàô $W^n\to \infty$ÔºåÂ¶ÇÊûú $|W|<1$ÔºåÂàô $W^n\to 0$
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20123.png)
        
        - Gradient clipping: Scale gradient if its norm is too big
            - To cope with exploding gradients
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20124.png)
            
        - Change RNN architecture
            - To cope with vanishing gradients
    - Long Short Term Memory (LSTM)
        
        Designed to help alleviate the problem of vanishing and exploding gradients
        
        - LSTM has two hidden states at every time step
            - $h_t$ the hidden state, will reveal to the outside world
            - $c_t$ the cell state
        - four gates
            - f: Forget gate, Whether to erase cell
            - i: Input gate, whether to write to cell
            - g: Gate gate (?), How much to write to cell
            - o: Output gate, How much to reveal cell
        
        $$
        \begin{align*}
        \begin{pmatrix}
        i \\
        f \\
        o \\
        g
        \end{pmatrix}
        &=
        \begin{pmatrix}
        \sigma \\
        \sigma \\
        \sigma \\
        \tanh
        \end{pmatrix}
        W
        \begin{pmatrix}
        h_{t-1} \\
        x_t
        \end{pmatrix}
        \\
        c_t& = f \odot c_{t-1} + i \odot g
        \\
        h_t& = o \odot \tanh(c_t)
        \end{align*}
        $$
        
        $\odot$ means the element wise product of two matrixs
        
        - Backprop
            
            Backpropagation from $c_t$ to $c_{t-1}$ only **elementwise
            multiplication** by $f$, no matrix multiply by $W$
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20125.png)
            
- Summary
    - RNNs allow a lot of flexibility in architecture design
    - Vanilla RNNs are simple but don‚Äôt work very well
    - Common to use LSTM or GRU: their additive interactions
    improve gradient flow
    - Backward flow of gradients in RNN can explode or vanish.
    Exploding is controlled with gradient clipping. Vanishing is
    controlled with additive interactions (LSTM)
    - Better/simpler architectures are a hot topic of current research
    - Better understanding (both theoretical and empirical) is needed.

## Lecture 11 Detection and Segmentation

### Image Classification

![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20126.png)

### Other Computer Vision Tasks

![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20127.png)

- Semantic Segmentation
    
    Label each pixel in the image with a category label
    Don‚Äôt differentiate instances (Âå∫ÂàÜÂÆû‰æã), only care about pixels
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20128.png)
    
    - Semantic Segmentation Idea
        - Sliding Window
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20129.png)
            
            Problem: Very inefficient! Not reusing shared features between overlapping patches
            
        - Fully Convolutional
            
            Design a network as a bunch of convolutional layers to make predictions for pixels all at once!
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20130.png)
            
            Design network as a bunch of convolutional layers, with downsampling and upsampling inside the network!
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20131.png)
            
            Downsampling: Pooling, strided convolution
            
            - Strided convolution (Ë∑®Ê≠•Âç∑ÁßØ)
                
                3 x 3 convolution, stride 2 pad 1
                
                ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20132.png)
                
                Filter moves 2 pixels in the input for every one pixel in the output
                
                Stride gives ratio between movement in input and
                output
                
            - In-Network upsampling: ‚ÄúUnpooling‚Äù
                
                ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20133.png)
                
                - Max Unpooling
                    
                    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20134.png)
                    
                - Tranpose Convolution
                    
                    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20135.png)
                    
                    Other names: 
                    
                    Deconvolution (bad), Upconvolution, Fractionally strided convolution, Backward strided convolution
                    
                    1D Example
                    
                    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20136.png)
                    
                    Convolution as Matrix Multiplication (1D Example)
                    
                    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20137.png)
                    
                    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20138.png)
                    
- Classification + Localization
    
    You have some **fix number of objects** that you are looking for
    
    Treat localization as a regression problem.
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20139.png)
    
    Aside: Human Pose Estimation
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20140.png)
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20141.png)
    
- Object Detection
    
    we don‚Äôt know how many object instances we‚Äôre looking for in the image ahead of time
    
    - Object Detection as Regression? X
    Each image needs a different number of outputs!
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20142.png)
        
    - Object Detection as Classification: Sliding Window
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20143.png)
        
        Problem: Need to apply CNN to huge number of locations and scales, very computationally expensive!
        
    - Region Proposals (ÂÄôÈÄâÂå∫Âüü)
        - Find ‚Äúblobby‚Äù image regions that are likely to contain objects
        - Relatively fast to run; e.g. Selective Search gives 1000 region proposals in a few seconds on CPU
        - very high recall
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20144.png)
        
        - R-CNN
            
            Region-based Convolutional Neural Networks
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20145.png)
            
            - Ad hoc training objectives
                - Fine-tune network with softmax classifier (log loss)
                - Train post-hoc linear SVMs (hinge loss)
                - Train post-hoc bounding-box regressions (least squares)
                - Training is slow (84h), takes a lot of disk space
                - Inference (detection) is slow
                - 47s / image with VGG16 [Simonyan & Zisserman. ICLR15]
                - Fixed by SPP-net [He et al. ECCV14]
        - Fast R-CNN
            
            Taking crops from the convolutional feature map corresponding to each proposal rather than taking crops directly from the image ‚Üí reuse a lot of expensive convolutional computation across the entire image
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20146.png)
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20147.png)
            
            Problem: Runtime dominated by region proposals! Computing the region proposals using the fixed function became a bottleneck.
            
        - Faster R-CNN
            
            Make CNN itself do proposals!
            Insert Region Proposal Network (RPN) to predict proposals from features
            
            - Jointly train with 4 losses:
                - RPN classify object / not object
                - RPN regress box coordinates
                - Final classification score (object classes)
                - Final box coordinates
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20148.png)
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20149.png)
            
    - Detection without Proposals: YOLO / SSD
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20150.png)
        
        Go from input image to tensor of scores with one big convolutional network!
        
        - Within each grid cell:
            - Regress from each of the B base boxes to a final box with 5 numbers: (dx, dy, dh, dw, confidence)
            - Predict scores for each of C classes (including background as a class)
        
        Output: 7 x 7 x (5 * B + C)
        
    - Faster R-CNN is slower but more accurate,
    - SSD is much faster but not as accurate
    - Aside: Object Detection + Captioning = Dense Captioning
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20151.png)
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20152.png)
        
- Instance Segmentation
    
    a hybrid between Semantic Segmentation and Object Detection
    
    - Mask R-CNN
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20153.png)
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20154.png)
        
    - Mask R-CNN Also does pose
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20155.png)
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20156.png)
        

## Lecture 12 Visualizing and Understanding

[Lecture 12 Visualizing and Understanding.pdf](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Lecture_12_Visualizing_and_Understanding.pdf)

## Lecture 13 Generative Models

### Unsupervised Learning

- Data: x Just data, no labels!
‚Üí Training data is cheap
- Goal: Learn some underlying hidden structure of the data
Holy grail: Solve unsupervised learning ‚Üí understand structure of visual world
- Examples: Clustering, dimensionality reduction, feature learning, density estimation, etc.
- Generative Models
    
    Given training data, generate new samples from same distribution
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20157.png)
    
    Want to learn $p_{model}(x)$ similar to $p_{data}(x)$
    
    Addresses density estimation, a core problem in unsupervised learning
    Several flavors
    
    - Explicit density estimation: explicitly define and solve for $p_{model}(x)$
    - Implicit density estimation: learn model that can sample from $p_{model}(x)$ w/o explicitly defining it
    - Why Generative Models?
        - Realistic samples for artwork, super-resolution, colorization, etc.
            
            ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20158.png)
            
        - Generative models of time-series data can be used for simulation and planning (reinforcement learning applications!)
        - Training generative models can also enable inference of  latent representations that can be useful as general features
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20159.png)
    

### PixelRNN and PixeICNN

- Fully visible belief network
    
    Explicit density model
    Use chain rule to decompose likelihood of an image x into product of 1-d distributions:
    
    $p(x) = \prod\limits_{i=1}^{n} p(x_i | x_1, ..., x_{i-1})$
    
    $p(x)$ is the likelihood of image x
    
    $p(x_i | x_1, ..., x_{i-1})$ is the probability of i‚Äôth pixel value given all previous pixels
    
    Then maximize likelihood of training data
    
    - ÂÖ®ÊòæÂºè‰ø°ÂøµÁΩëÁªú
        
        ËøôÊòØ‰∏ÄÂº†ÂÖ≥‰∫éÂÖ®ÊòæÂºè‰ø°ÂøµÁΩëÁªúÔºàFully visible belief networkÔºâÁöÑÂõæÁâáÔºåÂÆÉÊòØ‰∏ÄÁßçÊ¶ÇÁéáÊ®°Âûã„ÄÇÂú®Ëøô‰∏™ÁΩëÁªú‰∏≠Ôºå‰ΩøÁî®ÈìæÂºèÊ≥ïÂàôÂ∞ÜÂõæÂÉèxÁöÑÊ¶ÇÁéáÂàÜËß£‰∏∫‰∏ÄÁ≥ªÂàóÂçïÁª¥ÂàÜÂ∏ÉÁöÑ‰πòÁßØ„ÄÇ
        
        ÂÖ∑‰ΩìÊù•ËØ¥ÔºåËØ•ÂõæÂ±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïÈÄöËøá‰ª•‰∏ãÊ≠•È™§Êù•ÊûÑÂª∫‰∏Ä‰∏™ÂÖ®ÊòæÂºè‰ø°ÂøµÁΩëÁªúÔºö
        
        1. ‰ΩøÁî®ÈìæÂºèÊ≥ïÂàôÂ∞ÜÂõæÂÉèxÁöÑÊ¶ÇÁéáp(x)Ë°®Á§∫‰∏∫ÊØè‰∏™ÂÉèÁ¥†ÂÄºxiÁöÑÊ¶ÇÁéá‰∏éÊâÄÊúâÂÖàÂâçÂÉèÁ¥†ÂÄºÁöÑÊù°‰ª∂Ê¶ÇÁéáÁöÑ‰πòÁßØ„ÄÇ
        2. ÂÆö‰πâ‚ÄúÂâç‰∏ÄÂÉèÁ¥†‚ÄùÁöÑÈ°∫Â∫èÔºå‰ª•‰æøËÆ°ÁÆóÁªôÂÆöÊâÄÊúâÂÖàÂâçÂÉèÁ¥†ÁöÑi'thÂÉèÁ¥†ÂÄºÁöÑÊ¶ÇÁéá„ÄÇ
        3. Â∞ÜÂ§çÊùÇÁöÑÂÉèÁ¥†ÂÄºÂàÜÂ∏ÉË°®Ëææ‰∏∫Á•ûÁªèÁΩëÁªúÁöÑÂΩ¢ÂºèÔºå‰ª•ÂÆûÁé∞ÂØπËÆ≠ÁªÉÊï∞ÊçÆÊúÄÂ§ß‰ººÁÑ∂ÊÄßÁöÑ‰ºòÂåñ„ÄÇ
        
        ÊúÄÂêéÔºåÈÄöËøáÊúÄÂ§ßÂåñËÆ≠ÁªÉÊï∞ÊçÆÁöÑ‰ººÁÑ∂ÊÄßÊù•Â≠¶‰π†ÁΩëÁªúÂèÇÊï∞Ôºå‰ªéËÄåÂæóÂà∞‰∏Ä‰∏™ËÉΩÂ§üÊèèËø∞ÂõæÂÉèxÁöÑÊ¶ÇÁéáÂàÜÂ∏ÉÁöÑÊ®°Âûã„ÄÇËøô‰∏™ËøáÁ®ãÈÄöÂ∏∏Áî®‰∫éÁîüÊàêÂºèÂª∫Ê®°‰ªªÂä°ÔºåÂ¶ÇÂõæÂÉèÁîüÊàêÊàñËØ≠Èü≥ÂêàÊàêÁ≠â„ÄÇ
        
- PixelRNN
    
    Generate image pixels starting from corner
    
    Dependency on previous pixels modeled using an RNN (LSTM)
    
    Drawback: sequential generation is slow!
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20160.png)
    
- PixelCNN
    
    Still generate image pixels starting from corner
    
    Dependency on previous pixels now modeled using a CNN over context region
    
    - Training: maximize likelihood of training images
        
        Âú®Êú∫Âô®Â≠¶‰π†ÂíåÁªüËÆ°Â≠¶‰∏≠Ôºå‚ÄúÊúÄÂ§ß‰ººÁÑ∂‚ÄùÊòØ‰∏ÄÁßç‰º∞ËÆ°Ê®°ÂûãÂèÇÊï∞ÁöÑÊñπÊ≥ï„ÄÇ
        
        ÂÖ∂Âü∫Êú¨ÊÄùÊÉ≥ÊòØÔºöÈÄâÊã©‰ΩøÂæóËßÇÂØüÊï∞ÊçÆÂá∫Áé∞ÁöÑÂèØËÉΩÊÄßÊúÄÂ§ßÁöÑÂèÇÊï∞ÂÄº„ÄÇÁÆÄÂçïÊù•ËØ¥ÔºåÂ∞±ÊòØÊâæÂà∞‰∏ÄÁªÑÂèÇÊï∞Ôºå‰ΩøÂæóÊàë‰ª¨Áî®Ëøô‰∫õÂèÇÊï∞ÊûÑÂª∫ÁöÑÊ®°ÂûãÈ¢ÑÊµãÂá∫Â∑≤Áü•ËßÇÊµãÁªìÊûúÁöÑÊ¶ÇÁéáÊúÄÈ´ò„ÄÇ 
        
        ÂØπ‰∫é PixelCNN ËøôÊ†∑ÁöÑÁîüÊàêÊ®°ÂûãÔºåÁõÆÊ†áÈÄöÂ∏∏ÊòØËÆ≠ÁªÉ‰∏Ä‰∏™ËÉΩÂ§üÂæàÂ•ΩÂú∞Ê®°ÊãüÁúüÂÆûÊï∞ÊçÆÂàÜÂ∏ÉÁöÑÊ®°Âûã„ÄÇÈÄöËøáÊúÄÂ§ßÂåñËÆ≠ÁªÉÈõÜ‰∏äÊØè‰∏™Ê†∑Êú¨ÁöÑËÅîÂêàÊ¶ÇÁéáÔºåÊàë‰ª¨ÂèØ‰ª•ÂæóÂà∞‰∏Ä‰∏™Êõ¥Êé•Ëøë‰∫éÁúüÂÆûÊï∞ÊçÆÂàÜÂ∏ÉÁöÑÊ®°Âûã„ÄÇËøôÊ†∑ÔºåÂú®ÁîüÊàêÊñ∞Ê†∑Êú¨Êó∂ÔºåÊ®°ÂûãÂèØ‰ª•‰∫ßÁîü‰∏éËÆ≠ÁªÉÊï∞ÊçÆÁõ∏‰ººÁöÑÁªìÊûú„ÄÇ PixelCNN ‰ΩøÁî®‰∫ÜÊù°‰ª∂Á•ûÁªèÁΩëÁªúÊù•Âª∫Ê®°ÊØè‰∏™ÂÉèÁ¥†ÂÄº‰∏éÂÖ∂‰ªñÂÉèÁ¥†‰πãÈó¥ÁöÑ‰æùËµñÂÖ≥Á≥ªÔºåÂπ∂‰∏îÈÄöËøá Softmax ÊçüÂ§±ÂáΩÊï∞ËøõË°å‰ºòÂåñ„ÄÇ
        
        Softmax ÂáΩÊï∞Â∞ÜËæìÂá∫Êò†Â∞ÑÂà∞‰∏Ä‰∏™Ê¶ÇÁéáÂàÜÂ∏ÉÔºå‰ΩøÂæóÊâÄÊúâÁöÑËæìÂá∫ÂÄºÈÉΩÂú® [0, 1] ËåÉÂõ¥ÂÜÖÔºåÂπ∂‰∏îÂÆÉ‰ª¨ÁöÑÊÄªÂíå‰∏∫ 1„ÄÇÂõ†Ê≠§ÔºåSoftmax ÊçüÂ§±ÂáΩÊï∞ÈÄöÂ∏∏Áî®‰∫éÂ§öÂàÜÁ±ªÈóÆÈ¢òÔºå‰ª•ÊúÄÂ∞èÂåñÈ¢ÑÊµãÊ¶ÇÁéáÂàÜÂ∏É‰∏éÂÆûÈôÖÊ†áÁ≠æÂàÜÂ∏É‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇ
        
         Âú® PixelCNN ÁöÑËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåÊ®°Âûã‰ºöÂ∞ùËØïË∞ÉÊï¥ÂèÇÊï∞Ôºå‰ΩøÂæóÂØπÊØè‰∏Ä‰∏™ËÆ≠ÁªÉÊ†∑Êú¨ÔºåÊ®°ÂûãÈÉΩËÉΩÁªôÂá∫ËæÉÈ´òÁöÑ‰ººÁÑ∂Â∫¶ÔºàÂç≥ËæÉÂ§ßÁöÑÈ¢ÑÊµãÊ¶ÇÁéáÔºâ„ÄÇ
        
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20161.png)
    
    Training is faster than PixelRNN (can parallelize convolutions since context region values known from training images)
    Generation must still proceed sequentially ‚Üí still slow
    
- recap
    - Pros
        - Can explicitly compute likelihood $p(x)$, which is an explicit density that we can optimize.
        - Explicit likelihood of training data gives good evaluation metric
        - Produce good samples
    - Con
        - Sequential generation ‚Üí slow

### Variational Autoencoders (VAE)

(ÂèòÂàÜËá™Âä®ÁºñÁ†ÅÂô®)

- Intro
    
    PixelCNNs define tractable density function, optimize likelihood of training data:
    
    $p_{\theta}(x) = \prod_{i=1}^{n} p_{\theta}(x_i | x_1, ..., x_{i-1})$
    
     VAEs define intractable density function with latent $z$: 
    
    $p_{\theta}(x) = \int p_{\theta}(z)p_{\theta}(x|z)dz$
    
    Cannot optimize directly, derive and optimize lower bound on likelihood instead
    
- Autoencoders
    
    Autoencoders is an unsupervised approach for learning a lower-dimensional feature representation from unlabeled training data
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20162.png)
    
    Q: Why dimensionality reduction from $z$ to $x$? 
    
    A: Want features to capture meaningful factors of variation in data
    
    - How to learn this feature representation?
        - Train such that features can be used to reconstruct original data ‚ÄúAutoencoding‚Äù - encoding itself
        - After training, throw away decoder
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20163.png)
        
- Variational Autoencoders
    
    Probabilistic spin on autoencoders ‚Üí will let us sample from the model to generate data!
    
    Assume training data $\left\{\mathbf{x}^{(i)}\right\}_{i=1}^N$ is generated from underlying unobserved (latent) representation $z$
    
    We want to estimate the true parameters of this generative model.
    
    - How should we represent this model?
        - Choose prior $p(z)$ to be simple, e.g. Gaussian. Reasonable for latent attributes, e.g. pose, how much smile.
        - Conditional $p(x|z)$ is complex (generates image) ‚Üí represent with neural network
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20164.png)
    
    How to train the model?
    Remember strategy for training generative models from FVBNs. Learn model parameters to maximize likelihood of training data $p_{\theta}(x) = \int p_{\theta}(z)p_{\theta}(x|z)dz$
    
    Q: What is the problem with this?
    A: Intractable!
    
    - Intractability
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20165.png)
        
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20166.png)
    
    Now equipped with our encoder and decoder networks, let‚Äôs work out the (log) data likelihood:
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20167.png)
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20168.png)
    
    Generating Data
    
    Use decoder network.  Now sample z from prior
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20169.png)
    
- recap
    
    Probabilistic spin to traditional autoencoders ‚Üí allows generating data
    Defines an intractable density ‚Üí derive and optimize a (variational) lower bound
    
    - Pros:
        - Principled approach to generative models
        - Allows inference of q(z|x), can be useful feature representation for other tasks
    - Cons:
        - Maximizes lower bound of likelihood: okay, but not as good evaluation as PixelRNN/PixelCNN
        - Samples blurrier and lower quality compared to state-of-the-art (GANs)
    - Active areas of research:
    - More flexible approximations, e.g. richer approximate posterior instead of diagonal Gaussian
    - Incorporating structure in latent variables

### Generative Adversarial Networks(GAN)

GANs: don‚Äôt work with any explicit density function! Instead, take game-theoretic (ÂçöÂºàËÆ∫) approach: learn to generate from training distribution through 2-player game

- Problem: Want to sample from complex, high-dimensional training distribution.  No direct way to do this!
- Solution: Sample from a simple distribution, e.g. random noise.
Learn transformation to training distribution.

Q: What can we use to represent this complex transformation?
A: A neural network!

![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20170.png)

- Training GANs: Two-player game
    - Generator network (ÁîüÊàêÁΩëÁªú) $G(z;\theta_{g})$: try to fool the discriminator by generating real-looking images
    - Discriminator network (Âà§Âà´ÁΩëÁªú) $D(x;\theta_{d})$: try to distinguish between real and fake images
    - Train jointly in minimax game
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20171.png)
    
    Minimax objective function:
    
    $\min\limits_{\theta_g}\max\limits_{\theta_d}\left[\mathbb{E}_{x\sim p_{data}}\log D_{\theta_d}(x)+\mathbb{E}_{z\sim p(z)}\log\left(1-D_{\theta_d}(G_{\theta_g}(z))\right)\right]$
    
    - $D_{\theta_d}(x)$: The discriminator output for real data $x$
    - $D_{\theta_d}(G_{\theta_g}(z))$: The discriminator output for generated fake data $G(z)$
    - Discriminator ($\theta_d$) wants to maximize objective such that $D(x)$ is close to 1 (real) and $D(G(z))$ is close to 0 (fake)
    - Generator ($\theta_g$) wants to minimize objective such that $D(G(z))$ is close to 1 (discriminator is fooled into thinking generated $G(z)$ is real)
    
    Alternate between
    
    - Gradient ascent on discriminator
    $\max\limits_{\theta_d}\left[\mathbb{E}_{x\sim p_{data}}\log D_{\theta_d}(x)+\mathbb{E}_{z\sim p(z)}\log\left(1-D_{\theta_d}(G_{\theta_g}(z))\right)\right]$
    - Gradient ascent on generator, different objective
    $\max\limits_{\theta_g}\mathbb{E}_{z\sim p(z)}\log\left(D_{\theta_d}(G_{\theta_g}(z))\right)$
    
    Instead of minimizing likelihood of discriminator being correct, now maximize likelihood of discriminator being wrong.
    Same objective of fooling discriminator, but now higher gradient
    signal for bad samples => works much better! Standard in practice.
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20172.png)
    
    Aside: Jointly training two networks is challenging, can be unstable.  Choosing objectives with better loss landscapes helps training, is an active area of research.
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20173.png)
    
    - Putting it together: GAN training algorithm
        
        ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20174.png)
        
    
    After training, use generator network to generate new images
    
- Recap
    
    Don‚Äôt work with an explicit density function
    Take game-theoretic approach: learn to generate from training distribution through 2-player game
    
    - Pros:
        - Beautiful, state-of-the-art samples!
    - Cons:
        - Trickier / more unstable to train
        - Can‚Äôt solve inference queries (Êé®ÁêÜÊü•ËØ¢) such as p(x), p(z|x)
    - Active areas of research:
        - Better loss functions, more stable training (Wasserstein GAN, LSGAN, many others)
        - Conditional GANs, GANs for all kinds of applications
- Recap
    
    Generative Models
    
    - PixelRNN and PixelCNN
    Explicit density model, optimizes exact likelihood, good
    samples. But inefficient because sequential generation.
    - Variational Autoencoders (VAE)
    Optimize variational lower bound on likelihood. Useful
    latent representation, inference queries (Êé®ÁêÜÊü•ËØ¢). But current sample quality not the best.
    - Generative Adversarial Networks (GANs)
    Game-theoretic approach, best samples! But can be tricky and unstable to train, no inference queries.

## Lecture 14 Reinforcement Learning

### Reinforcement Learning

Problems involving an agent interacting with an environment,
which provides numeric reward signals
Goal: Learn how to take actions in order to maximize reward

![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20175.png)

- Example
    
    Cart-Pole Problem
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20176.png)
    
    Robot Locomotion
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20177.png)
    
    Atari Games
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20178.png)
    
    Go
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20179.png)
    

### Markov Decision Processes

- Mathematical formulation of the RL problem
- Markov property: Current state completely characterises the state of the
world
- Defined by tuple:  $(S,A,R,P,\gamma)$
    - $S$: set of possible states
    - $A$: set of possible actions
    - $R$: distribution of reward given (state, action) pair
    - $P$: transition probability i.e. distribution over next state given (state, action) pair
    - $\gamma$: discount factor
- Markov Decision Process
    - At time step t=0, environment samples initial state s0 ~ p(s0)
    - Then, for t=0 until done:
        - Agent selects action at
        - Environment samples reward $r_t ~ R(\cdot| s_t, a_t)$
        - Environment samples next state $s_{t+1}~ P( \cdot| s_t, a_t)$
        - Agent receives reward $r_t$ tand next state $s_{t+1}$
    - A policy $\pi$ is a function from S to A, that specifies what action to take in each state
    - Objective: find policy $\pi^*$ that maximizes cumulative discounted reward: $\sum\limits_{t \geq 0} \gamma^t r_t$
- Example
    
     Grid World
    
    ![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20180.png)
    
- The optimal policy $\pi^*$
    
    We want to find optimal policy $\pi^*$ that maximizes the sum of rewards.
    How do we handle the randomness (initial state, transition probability‚Ä¶)?
    Maximize the expected sum of rewards. Formally:
    
    $\pi^{*} = \arg\max\limits_{\pi} \mathbb{E}\left[\sum\limits_{t \geq 0} \gamma^tr_t | \pi \right] \quad \text{with} \quad s_0 \sim p(s_0), a_t \sim \pi(\cdot|s_t), s_{t+1} \sim p(\cdot|s_t, a_t)$
    
- Value function and Q-value function
    
    Following a policy produces sample trajectories (or paths)  s0 , a0 , r0 , s1 , a1 , r1 , ‚Ä¶
    
    How good is a state?
    The value function at state $s$, is the expected cumulative reward from following the policy
    from state s:
    
    $V^\pi(s) = \mathbb{E}\left[\sum\limits_{t \geq 0} \gamma^tr_t | s_0 = s, \pi\right]$
    
    How good is a state-action pair?
    The Q-value function at state $s$ and action $a$, is the expected cumulative reward from
    taking action $a$ in state $s$ and then following the policy:
    
    $Q^\pi(s,a) = \mathbb{E}\left[\sum\limits_{t \geq 0} \gamma^tr_t | s_0 = s, a_0 = a, \pi\right]$
    
    - Bellman equation
    The optimal Q-value function $Q^*$ is the maximum expected cumulative reward achievable
    from a given (state, action) pair:
        
        $Q^*(s,a) = \max\limits_\pi\mathbb{E}\left[\sum\limits_{t \geq 0} \gamma^tr_t | s_0 = s, a_0 = a, \pi\right]$
        
        $Q^*$ satisfies the following Bellman equation:
        
        $Q^*(s, a) = \mathbb{E}_{s' \sim \varepsilon} \left[ r + \gamma \max\limits_{a'} Q^*(s', a') \middle| s, a \right]$
        

Q-Learning

Policy Gradients

### Question

![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20181.png)

[lp spaces - What is rotation when we have a different distance metric? - Mathematics Stack Exchange](https://math.stackexchange.com/questions/2057140/what-is-rotation-when-we-have-a-different-distance-metric?answertab=votes#tab-top)

![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20182.png)

![Untitled](CS231n%2050ef35bb76354c35bec4f4fc500d8c2e/Untitled%20183.png)

[7.6. ÊÆãÂ∑ÆÁΩëÁªúÔºàResNetÔºâ ‚Äî Âä®ÊâãÂ≠¶Ê∑±Â∫¶Â≠¶‰π† 2.0.0 documentation (d2l.ai)](https://zh.d2l.ai/chapter_convolutional-modern/resnet.html#id1)